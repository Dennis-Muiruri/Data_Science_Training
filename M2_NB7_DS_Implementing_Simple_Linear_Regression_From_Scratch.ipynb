{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Implementing Simple Linear Regression From Scratch With Python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this tutorial, you will discover how to implement the simple linear regression algorithm from scratch in Python.</p><p>After completing this tutorial you will know:</p><ul>\n",
    "<li>How to estimate statistical quantities from training data.</li>\n",
    "<li>How to estimate linear regression coefficients from data.</li>\n",
    "<li>How to make predictions using linear regression for new data.</li>\n",
    "</ul><h3>Simple Linear Regression</h3><p>Linear regression assumes a linear or straight line relationship between the input variables (X) and the single output variable (y).</p><p>More specifically, that output (y) can be calculated from a linear combination of the input variables (X). When there is a single input variable, the method is referred to as a simple linear regression.</p><p>In simple linear regression we can use statistics on the training data to estimate the coefficients required by the model to make predictions on new data.</p><p>The line for a simple linear regression model can be written as:</p><p>y = b0 + b1 * x</p><p>where b0 and b1 are the coefficients we must estimate from the training data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Once the coefficients are known, we can use this equation to estimate output values for y given new input examples of x.</p><p>It requires that you calculate statistical properties from the data such as mean, variance and covariance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>All the algebra has been taken care of and we are left with some arithmetic to implement to estimate the simple linear regression coefficients.</p><p>Briefly, we can estimate the coefficients as follows:</p><p></p><p>$$ B_1 = \\frac{\\sum_{i} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i} (x_i - \\bar{x})^2} $$</p><p></p>$$ B_0 = \\bar{y} - B_1 \\times \\bar{x} $$</p><p>where the i refers to the value of the ith value of the input x or output y.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Swedish Insurance Dataset</h3><p>We will use a real dataset to demonstrate simple linear regression.</p><p>The dataset is called the “Auto Insurance in Sweden” dataset and involves predicting the total payment for all the claims in thousands of Swedish Kronor (y) given the total number of claims (x).</p><p>This means that for a new number of claims (x) we will be able to predict the total payment of claims (y).</p><p>Here is a small sample of the first 5 records of the dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "df = pd.read_csv('insurance.csv', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63 entries, 0 to 62\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       63 non-null     int64  \n",
      " 1   1       63 non-null     float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 1.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Rule Algorithm\n",
    "The Zero Rule Algorithm, often referred to as the ZeroR algorithm, is one of the simplest classification algorithms in machine learning. It serves as a basic baseline method to compare with other, more complex algorithms. Its name derives from the fact that it involves zero rules for predicting.\n",
    "\n",
    "Here's how it works for different types of problems:\n",
    "\n",
    "1. **Classification**: \n",
    "    - For a classification problem, the Zero Rule Algorithm predicts the class that is most frequent in the training dataset. In other words, if you're trying to classify emails into \"spam\" or \"not spam\", and 80% of your training set is \"not spam\", the ZeroR algorithm will classify all emails as \"not spam\".\n",
    "\n",
    "2. **Regression**:\n",
    "    - For a regression problem, the Zero Rule Algorithm predicts the mean (or median, depending on the preference or specific use-case) output value in the training dataset.\n",
    "\n",
    "The Zero Rule Algorithm provides a naive baseline, and any sophisticated algorithm should ideally perform better than this baseline. If not, there might be issues with that particular algorithm or with the dataset's preprocessing and handling.\n",
    "\n",
    "While the ZeroR algorithm might sound overly simplistic, it's particularly useful in scenarios where the dataset is highly imbalanced. Knowing the naive baseline helps in gauging how well an advanced algorithm performs relative to a simple guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE\n",
    "RMSE stands for Root Mean Square Error. It's a frequently used metric to evaluate the differences between values predicted by a model or an estimator and the values actually observed. It provides a measure of the overall error between predicted and observed values for a given dataset.\n",
    "\n",
    "Mathematically, RMSE is defined as:\n",
    "\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "Where:\n",
    "- \\( n \\) is the number of observations.\n",
    "- \\( y_i \\) is the actual value for the i-th observation.\n",
    "<p>$$ \\hat{y}_i $$ is the predicted value for the i-th observation.</p>\n",
    "\n",
    "Key points about RMSE:\n",
    "\n",
    "1. **Scale**: RMSE is in the same unit as the quantity being estimated. For example, if you're predicting house prices (in thousands of dollars), then the RMSE would also be in thousands of dollars.\n",
    "\n",
    "2. **Magnitude**: A lower RMSE indicates a better fit of the model to the data, as it signifies that the model's predictions are close to the observed values. However, the \"goodness\" of the RMSE value depends on the context and the domain. An RMSE of $10,000 might be acceptable when predicting house prices, but not when predicting the price of a book.\n",
    "\n",
    "3. **Sensitivity**: RMSE gives a relatively high weight to large errors because the differences are squared before they are averaged. This means that RMSE is more sensitive to occasional large errors than Mean Absolute Error (MAE).\n",
    "\n",
    "4. **Comparison**: RMSE is useful when comparing different models on the same dataset. A model with a lower RMSE is generally considered better, as long as the comparison is being done on the same dataset.\n",
    "\n",
    "In many applications, particularly in regression problems, RMSE serves as a key metric to evaluate model performance and guide the tuning of model hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Using the Zero Rule algorithm (that predicts the mean value) a Root Mean Squared Error or RMSE of about 81 (thousands of Kronor) is expected.</p><p>Below is a scatter plot of the entire dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = df.plot.scatter(x = 0, y= 1, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Notebook</h2><p>This notebook is broken down into five parts:</p><ol>\n",
    "<li>Calculate Mean and Variance.</li>\n",
    "<li>Calculate Covariance.</li>\n",
    "<li>Estimate Coefficients.</li>\n",
    "<li>Make Predictions.</li>\n",
    "<li>Predict Insurance.</li>\n",
    "</ol><p>These steps will give you the foundation you need to implement and train simple linear regression models for your own prediction problems.</p><h3>1. Calculate Mean and Variance</h3><p>The first step is to estimate the mean and the variance of both the input and output variables from the training data.</p><p>The <strong>mean</strong> of a list of numbers can be calculated as:</p><p></p>$$ \\text{Mean} (\\bar{X}) = \\frac{1}{n} \\sum_{i=1}^{n} x_i $$</p>\n",
    "\n",
    "Where:\n",
    "\n",
    "- $$ n $$ is the number of observations or data points.\n",
    "- $$ x_i $$ represents each individual data point in the set.\n",
    "\n",
    "<p>Below is a function named <strong>mean()</strong> that implements this behavior for a list of numbers.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value of a list of numbers\n",
    "def mean(values):\n",
    "    return sum(values) / float(len(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Variance is a statistical measure that describes the spread or dispersion of a set of data points. In other words, it quantifies how much each data point in a given dataset varies from the mean (average) of the dataset.</p><p><strong>Variance</strong> for a list of numbers can be calculated as:</p><p></p><p>$$\\text{variance}(\\sigma^2) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2 $$</p>\n",
    "where:\n",
    "\n",
    "- $$ N $$ : is the number of observations in the population.\n",
    "- $$ x_i $$: represents each individual data point.\n",
    "- $$ \\mu $$: is the mean of the population.\n",
    "- $$ \\sum $$ : denotes summation.\n",
    "\n",
    "<p>Below is a function named <strong>variance()</strong> that calculates the <a href=\"https://en.wikipedia.org/wiki/Simple_linear_regression\">sample variance</a> of a list of numbers (<strong>Note</strong> that we are intentionally calculating the sum squared difference from the mean, instead of the average squared difference from the mean). It requires the mean of the list to be provided as an argument, just so we don’t have to calculate it more than once.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance of a list of numbers\n",
    "def variance(values, mean):\n",
    "    \"\"\"\n",
    "    Compute the variance for a list of values and their mean.\n",
    "\n",
    "    Parameters:\n",
    "    - values (list of float): The data values.\n",
    "    - mean_value (float): The mean of the data values.\n",
    "\n",
    "    Returns:\n",
    "    - float: The variance of the data values.\n",
    "    \"\"\"\n",
    "    n = len(values)\n",
    "    variance_value = sum([(x - mean)**2 for x in values]) / n\n",
    "    return variance_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can put these two functions together and test them on the Swedish Insurance dataset. We can calculate the mean and variance for both the x and y values in the example below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x stats: mean=22.905 variance=536.658\n",
      "y stats: mean=98.187 variance=7505.052\n"
     ]
    }
   ],
   "source": [
    "dataset = df.values\n",
    "x = [row[0] for row in dataset]\n",
    "y = [row[1] for row in dataset]\n",
    "mean_x, mean_y = mean(x), mean(y)\n",
    "var_x, var_y = variance(x, mean_x), variance(y, mean_y)\n",
    "print('x stats: mean=%.3f variance=%.3f' % (mean_x, var_x))\n",
    "print('y stats: mean=%.3f variance=%.3f' % (mean_y, var_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running this example prints out the mean and variance for both columns.</p><p>This is our first step, next we need to put these values to use in calculating the covariance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate Covariance\n",
    "Covariance is a statistical measure that indicates the extent to which two variables change in tandem relative to their means. In other words, it provides insights into how two variables vary together.\n",
    "\n",
    "Mathematically, the covariance between two variables \\(X\\) and \\(Y\\) is defined as:\n",
    "\n",
    "$$ \\text{cov}(X, Y) = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) $$\n",
    "\n",
    "\n",
    "Where:\n",
    "- \\( n \\): is the number of data points.\n",
    "- \\( Xi \\) and \\( Yi \\): are individual data points.\n",
    "\n",
    "Key points about covariance:\n",
    "\n",
    "1. **Sign**:\n",
    "   - A **positive covariance** means that as one variable increases, the other also tends to increase, and as one decreases, the other also tends to decrease.\n",
    "   - A **negative covariance** means that as one variable increases, the other tends to decrease, and vice versa.\n",
    "   - A covariance close to **zero** suggests that the variables are less linearly related.\n",
    "\n",
    "2. **Units**: The unit of covariance is the product of the units of the two variables, which can sometimes be challenging to interpret. For instance, if you're finding the covariance between age (years) and height (cm), the unit of covariance would be years multiplied by centimeters.\n",
    "\n",
    "3. **Limitations**: While covariance indicates the direction of a linear relationship between two variables, it doesn't give any information about the strength of that relationship. For understanding both direction and strength, the **correlation coefficient** (like Pearson's \\( r \\)) is often used, as it scales the relationship to be between -1 and 1.\n",
    "\n",
    "Covariance is a generalization of correlation. Correlation describes the relationship between two groups of numbers, whereas covariance can describe the relationship between two or more groups of numbers. In many statistical analyses and in finance, covariance plays a critical role, especially in understanding how different assets in a portfolio might move in relation to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Below is a function named <strong>covariance()</strong> that implements this statistic. It builds upon the previous step and takes the lists of x and y&nbsp;values as well as the mean of these values as arguments.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(x, mean_x, y, mean_y):\n",
    "    \"\"\"\n",
    "    Compute the covariance for two lists of values and their means.\n",
    "\n",
    "    Parameters:\n",
    "    - x (list of float): Data values for X.\n",
    "    - mean_x (float): Mean of the data values for X.\n",
    "    - y (list of float): Data values for Y.\n",
    "    - mean_y (float): Mean of the data values for Y.\n",
    "\n",
    "    Returns:\n",
    "    - float: The covariance of X and Y.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    covar = sum([(x[i] - mean_x) * (y[i] - mean_y) for i in range(n)]) / n\n",
    "    return covar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Putting it all together we get the example below. We can test the calculation of the covariance  on the Swedish Insurance dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance: 1832.054\n"
     ]
    }
   ],
   "source": [
    "# calculate covariance\n",
    "dataset = df.values\n",
    "x = [row[0] for row in dataset]\n",
    "y = [row[1] for row in dataset]\n",
    "mean_x, mean_y = mean(x), mean(y)\n",
    "covar = covariance(x, mean_x, y, mean_y)\n",
    "print('Covariance: %.3f' % (covar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running this example prints the covariance for the x and y variables.</p><p>We now have all the pieces in place to calculate the coefficients for our model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Estimate Coefficients</h3><p>We must estimate the values for two coefficients in simple linear regression.</p><p>The first is B1 which can be estimated as:</p><p><p>$$ B_1 = \\frac{\\sum_{i} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i} (x_i - \\bar{x})^2} $$</p><p>We have learned some things above and can simplify this arithmetic to:</p><p></p><p>B1 = covariance(x, y) / variance(x)</p><p>We already have functions to calculate <strong>covariance()</strong> and <strong>variance()</strong>.</p><p>Next, we need to estimate a value for B0, also called the intercept as it controls the starting point of the line where it intersects the y-axis.</p><p></p><p>$$ B_0 = \\bar{y} - B_1 \\times \\bar{x} $$</p><p>Again, we know how to estimate B1 and we have a function to estimate <strong>mean()</strong>.</p><p>We can put all of this together into a function named <strong>coefficients()</strong> that takes the dataset as an argument and returns the coefficients.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coefficients\n",
    "def coefficients(dataset):\n",
    "    x = [row[0] for row in dataset]\n",
    "    y = [row[1] for row in dataset]\n",
    "    x_mean, y_mean = mean(x), mean(y)\n",
    "    b1 = covariance(x, x_mean, y, y_mean) / variance(x, x_mean)\n",
    "    b0 = y_mean - b1 * x_mean\n",
    "    return [b0, b1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can put this together with all of the functions from the previous two steps and test out the calculation of coefficients.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: B0=19.994, B1=3.414\n"
     ]
    }
   ],
   "source": [
    "# Calculate Coefficients\n",
    " \n",
    "# Calculate the mean value of a list of numbers\n",
    "def mean(values):\n",
    "    return sum(values) / float(len(values))\n",
    " \n",
    "# Calculate covariance between x and y\n",
    "def covariance(x, mean_x, y, mean_y):\n",
    "    n = len(x)\n",
    "    covar = sum([(x[i] - mean_x) * (y[i] - mean_y) for i in range(n)]) / n\n",
    "    return covar\n",
    " \n",
    "# Calculate the variance of a list of numbers\n",
    "def variance(values, mean):\n",
    "    n = len(values)\n",
    "    variance_value = sum([(x - mean)**2 for x in values]) / n\n",
    "    return variance_value\n",
    " \n",
    "# Calculate coefficients\n",
    "def coefficients(dataset):\n",
    "    x = [row[0] for row in dataset]\n",
    "    y = [row[1] for row in dataset]\n",
    "    x_mean, y_mean = mean(x), mean(y)\n",
    "    b1 = covariance(x, x_mean, y, y_mean) / variance(x, x_mean)\n",
    "    b0 = y_mean - b1 * x_mean\n",
    "    return [b0, b1]\n",
    " \n",
    "# calculate coefficients\n",
    "dataset =df.values\n",
    "b0, b1 = coefficients(dataset)\n",
    "print('Coefficients: B0=%.3f, B1=%.3f' % (b0, b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running this example calculates and prints the coefficients.</p><p>Now that we know how to estimate the coefficients, the next step is to use them.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Make Predictions</h3><p>The simple linear regression model is a line defined by coefficients estimated from training data.</p><p>Once the coefficients are estimated, we can use them to make predictions.</p><p>The equation to make predictions with a simple linear regression model is as follows:</p><p></p><p>y = b0 + b1 * x</p><p>Below is a function named <strong>simple_linear_regression()</strong> that implements the prediction equation to make predictions on a test dataset. It also ties together the estimation of the coefficients on training data from the steps above.</p><p>The coefficients prepared from the training data are used to make predictions on the test data, which are then returned.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_regression(train, test):\n",
    "    predictions = []\n",
    "    b0, b1 = coefficients(train)\n",
    "    for row in test:\n",
    "        yhat = b0 + b1 * row[0]\n",
    "        predictions.append(yhat)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let’s pull together everything we have learned and make predictions for our simple contrived dataset.</p><p>As part of this example, we will also add in a function to manage the evaluation of the predictions called <strong>evaluate_algorithm()</strong> and another function to estimate the Root Mean Squared Error of the predictions called <strong>rmse_metric()</strong>.</p><p>The full example is listed below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standalone simple linear regression example\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "    \"\"\"\n",
    "    Compute the root mean squared error (RMSE).\n",
    "\n",
    "    Parameters:\n",
    "    - actual (list of float): Actual data values.\n",
    "    - predicted (list of float): Predicted data values.\n",
    "\n",
    "    Returns:\n",
    "    - float: The RMSE between actual and predicted values.\n",
    "    \"\"\"\n",
    "    n = len(actual)\n",
    "    mean_error = sum([(actual[i] - predicted[i])**2 for i in range(n)]) / n\n",
    "    return sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate regression algorithm on training dataset\n",
    "#use other algorithm\n",
    "def evaluate_algorithm(dataset, algorithm):\n",
    "    test_set = []\n",
    "    for row in dataset:\n",
    "        row_copy = list(row)\n",
    "        row_copy[-1] = None\n",
    "        test_set.append(row_copy)\n",
    "    predicted = algorithm(dataset, test_set)\n",
    "    print(predicted)\n",
    "    actual = [row[-1] for row in dataset]\n",
    "    rmse = rmse_metric(actual, predicted)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value of a list of numbers\n",
    "def mean(values):\n",
    "    return sum(values) / float(len(values))\n",
    " \n",
    "# Calculate covariance between x and y\n",
    "def covariance(x, mean_x, y, mean_y):\n",
    "    n = len(x)\n",
    "    covar = sum([(x[i] - mean_x) * (y[i] - mean_y) for i in range(n)]) / n\n",
    "    return covar\n",
    " \n",
    "# Calculate the variance of a list of numbers\n",
    "def variance(values, mean):\n",
    "    n = len(values)\n",
    "    variance_value = sum([(x - mean)**2 for x in values]) / n\n",
    "    return variance_value\n",
    " \n",
    "# Calculate coefficients\n",
    "def coefficients(dataset):\n",
    "    x = [row[0] for row in dataset]\n",
    "    y = [row[1] for row in dataset]\n",
    "    x_mean, y_mean = mean(x), mean(y)\n",
    "    b1 = covariance(x, x_mean, y, y_mean) / variance(x, x_mean)\n",
    "    b0 = y_mean - b1 * x_mean\n",
    "    return [b0, b1]\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1999999999999995, 1.9999999999999996, 3.5999999999999996, 2.8, 4.3999999999999995]\n",
      "RMSE: 0.693\n"
     ]
    }
   ],
   "source": [
    "# Simple linear regression algorithm\n",
    "def simple_linear_regression(train, test):\n",
    "    predictions = []\n",
    "    b0, b1 = coefficients(train)\n",
    "    for row in test:\n",
    "        yhat = b0 + b1 * row[0]\n",
    "        predictions.append(yhat)\n",
    "    return predictions\n",
    " \n",
    "# Test simple linear regression\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "#dataset = df.values\n",
    "rmse = evaluate_algorithm(dataset, simple_linear_regression)\n",
    "print('RMSE: %.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running this example displays the above output that first lists the predictions and&nbsp;the RMSE of these predictions.</p><p>Finally, we can plot the predictions as a line and compare it to the original dataset.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Predict Insurance</h3><p>We now know how to implement a simple linear regression model.</p><p>Let’s apply it to the Swedish insurance dataset.</p><p>This section assumes that you have downloaded the dataset to the file <strong>insurance.csv</strong> and it is available in the current working directory.</p><p>We will add some convenience functions to the simple linear regression from the previous steps.</p><p>Specifically a function to load the CSV file called <strong>load_csv()</strong>, a function to convert a loaded dataset to numbers called <strong>str_column_to_float()</strong>, a function to evaluate an algorithm using a train and test set called <strong>train_test_split()</strong> a function to calculate RMSE called <strong>rmse_metric()</strong> and a function to evaluate an algorithm called <strong>evaluate_algorithm()</strong>.</p><p>The complete example is listed below.</p><p>A training dataset of 60% of the data is used to prepare the model and predictions are made on the remaining 40%.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression on the Swedish Insurance Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into a train and test set\n",
    "def train_test_split(dataset, split):\n",
    "    train = []\n",
    "    train_size = split * len(dataset)\n",
    "    dataset_copy = list(dataset)\n",
    "    while len(train) < train_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        train.append(dataset_copy.pop(index))\n",
    "    return train, dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "    n = len(actual)\n",
    "    mean_error = sum([(actual[i] - predicted[i])**2 for i in range(n)]) / n\n",
    "    return sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a train/test split\n",
    "def evaluate_algorithm(dataset, algorithm, split, *args):\n",
    "    train, test = train_test_split(dataset, split)\n",
    "    test_set = []\n",
    "    for row in test:\n",
    "        row_copy = list(row)\n",
    "        row_copy[-1] = None\n",
    "        test_set.append(row_copy)\n",
    "    predicted = algorithm(train, test_set, *args)\n",
    "    actual = [row[-1] for row in test]\n",
    "    rmse = rmse_metric(actual, predicted)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value of a list of numbers\n",
    "def mean(values):\n",
    "    return sum(values) / float(len(values))\n",
    " \n",
    "# Calculate covariance between x and y\n",
    "def covariance(x, mean_x, y, mean_y):\n",
    "    n = len(x)\n",
    "    covar = sum([(x[i] - mean_x) * (y[i] - mean_y) for i in range(n)]) / n\n",
    "    return covar\n",
    " \n",
    "# Calculate the variance of a list of numbers\n",
    "def variance(values, mean):\n",
    "    n = len(values)\n",
    "    variance_value = sum([(x - mean)**2 for x in values]) / n\n",
    "    return variance_value\n",
    " \n",
    "# Calculate coefficients\n",
    "def coefficients(dataset):\n",
    "    x = [row[0] for row in dataset]\n",
    "    y = [row[1] for row in dataset]\n",
    "    x_mean, y_mean = mean(x), mean(y)\n",
    "    b1 = covariance(x, x_mean, y, y_mean) / variance(x, x_mean)\n",
    "    b0 = y_mean - b1 * x_mean\n",
    "    return [b0, b1]\n",
    "\n",
    "# Simple linear regression algorithm\n",
    "def simple_linear_regression(train, test):\n",
    "    predictions = []\n",
    "    b0, b1 = coefficients(train)\n",
    "    for row in test:\n",
    "        yhat = b0 + b1 * row[0]\n",
    "        predictions.append(yhat)\n",
    "    return predictions\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 33.630\n"
     ]
    }
   ],
   "source": [
    "# Simple linear regression on insurance dataset\n",
    "# The seed method is used to initialize the pseudorandom number generator in Python.\n",
    "# If you provide same seed value before generating random data it will produce the same data.\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'insurance.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# evaluate algorithm\n",
    "split = 0.6\n",
    "rmse = evaluate_algorithm(dataset, simple_linear_regression, split)\n",
    "print('RMSE: %.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[388.6874302462824, 84.85713340037579, 64.37419203997757, 443.3086072073443, 156.5474281617695, 214.58242868289773, 98.51242764064125, 67.78801560004395, 173.61654596210133, 54.13272135977848, 37.06360355944665, 183.8580166423004, 57.54654491984485, 98.51242764064125, 43.89125067957938, 26.822132879247544, 101.92625120070761, 40.47742711951301, 30.23595643931391, 98.51242764064125, 40.47742711951301, 50.71889779971211, 50.71889779971211, 30.23595643931391, 118.99536900103945, 43.89125067957938, 33.64977999938028, 88.27095696044215, 43.89125067957938, 33.64977999938028, 19.99448575911481, 105.34007476077399, 40.47742711951301, 37.06360355944665, 95.09860408057489, 57.54654491984485, 228.23772292316318, 60.96036847991121, 33.64977999938028, 74.61566272017669, 64.37419203997757, 224.8238993630968, 159.96125172183585, 146.30595748157037, 207.75478156276498, 159.96125172183585, 57.54654491984485, 112.16772188090671, 47.30507423964575, 30.23595643931391, 78.02948628024305, 64.37419203997757, 64.37419203997757, 71.20183916011031, 47.30507423964575, 118.99536900103945, 122.40919256110581, 101.92625120070761, 50.71889779971211, 125.82301612117219, 67.78801560004395, 200.92713444263225, 108.75389832084035]\n",
      "RMSE: 35.366\n"
     ]
    }
   ],
   "source": [
    "# Evaluate regression algorithm on training dataset\n",
    "def evaluate_algorithm(dataset, algorithm):\n",
    "    test_set = []\n",
    "    for row in dataset:\n",
    "        row_copy = list(row)\n",
    "        row_copy[-1] = None\n",
    "        test_set.append(row_copy)\n",
    "    predicted = algorithm(dataset, test_set)\n",
    "    print(predicted)\n",
    "    actual = [row[-1] for row in dataset]\n",
    "    rmse = rmse_metric(actual, predicted)\n",
    "    return rmse\n",
    "\n",
    "# load and prepare data\n",
    "filename = 'insurance.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "rmse = evaluate_algorithm(dataset, simple_linear_regression)\n",
    "print('RMSE: %.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=392.500, Predicted=388.687\n",
      "Expected=46.200, Predicted=84.857\n",
      "Expected=15.700, Predicted=64.374\n",
      "Expected=422.200, Predicted=443.309\n",
      "Expected=119.400, Predicted=156.547\n",
      "Expected=170.900, Predicted=214.582\n",
      "Expected=56.900, Predicted=98.512\n",
      "Expected=77.500, Predicted=67.788\n",
      "Expected=214.000, Predicted=173.617\n",
      "Expected=65.300, Predicted=54.133\n",
      "Expected=20.900, Predicted=37.064\n",
      "Expected=248.100, Predicted=183.858\n",
      "Expected=23.500, Predicted=57.547\n",
      "Expected=39.600, Predicted=98.512\n",
      "Expected=48.800, Predicted=43.891\n"
     ]
    }
   ],
   "source": [
    "#### Simple linear regression on insurance dataset\n",
    "# The seed method is used to initialize the pseudorandom number generator in Python.\n",
    "# If you provide same seed value before generating random data it will produce the same data.\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'insurance.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "coef = coefficients(dataset)\n",
    "for row in dataset[:15]:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running the algorithm prints the RMSE for the trained model on the training dataset.</p><p>A score of about 33 (thousands of Kronor) was achieved, which is much better than the Zero Rule algorithm that achieves approximately 81 (thousands of Kronor) on the same problem.</p><h2>Extensions</h2><p>The best extension to this tutorial is to try out the algorithm on more problems.</p><p>Small datasets with just an input (x) and output (y) columns are popular for demonstration in statistical books and courses. Many of these datasets are available online.</p><p>Seek out some more small datasets and make predictions using simple linear regression.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
