{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Modeling for the Imbalanced Classification of Good and Bad Credit</h1><p>Misclassification errors on the minority class are more important than other types of prediction errors for some imbalanced classification tasks. \n",
    "\n",
    "One example is the problem of classifying bank customers as to whether they should receive a loan or not. Giving a loan to a bad customer marked as a good customer results in a greater cost to the bank than denying a loan to a good customer marked as a bad customer.</p><p>This requires careful selection of a performance metric that both promotes minimizing misclassification errors in general, and favors minimizing one type of misclassification error over another.</p><p>The <strong>German credit dataset</strong> is a standard imbalanced classification dataset that has this property of differing costs to misclassification errors. Models evaluated on this dataset can be evaluated using the <a href=\"#\">Fbeta-Measure</a> that provides a way of both quantifying model performance generally, and captures the requirement that one type of misclassification error is more costly than another.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>German Credit Dataset</h2><p>In this project, we will use a standard imbalanced machine learning dataset referred to as the “<a href=\"https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)\">German Credit</a>” dataset or simply “<em>German</em>.”</p><p>The dataset was used as part of the Statlog project, a European-based initiative in the 1990s to evaluate and compare a large number (at the time) of machine learning algorithms on a range of different classification tasks. The dataset is credited to Hans Hofmann.</p><p>The german credit dataset describes financial and banking details for customers and the task is to determine whether the customer is good or bad. The assumption is that the task involves predicting whether a customer will pay back a loan or credit.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The dataset includes 1,000 examples and 20 input variables, 7 of which are numerical (integer) and 13 are categorical.</p><ul>\n",
    "<li>Status of existing checking account</li>\n",
    "<li>Duration in month</li>\n",
    "<li>Credit history</li>\n",
    "<li>Purpose</li>\n",
    "<li>Credit amount</li>\n",
    "<li>Savings account</li>\n",
    "<li>Present employment since</li>\n",
    "<li>Installment rate in percentage of disposable income</li>\n",
    "<li>Personal status and sex</li>\n",
    "<li>Other debtors</li>\n",
    "<li>Present residence since</li>\n",
    "<li>Property</li>\n",
    "<li>Age in years</li>\n",
    "<li>Other installment plans</li>\n",
    "<li>Housing</li>\n",
    "<li>Number of existing credits at this bank</li>\n",
    "<li>Job</li>\n",
    "<li>Number of dependents</li>\n",
    "<li>Telephone</li>\n",
    "<li>Foreign worker</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Some of the categorical variables have an ordinal relationship, such as “<em>Savings account</em>,” although most do not.</p><p>There are two classes, 1 for good customers and 2 for bad customers. Good customers are the default or negative class, whereas bad customers are the exception or positive class. A total of 70 percent of the examples are good customers, whereas the remaining 30 percent of examples are bad customers.</p><ul>\n",
    "<li><strong>Good Customers</strong>: Negative or majority class (70%).</li>\n",
    "<li><strong>Bad Customers</strong>: Positive or minority class (30%).</li>\n",
    "</ul><p>A cost matrix is provided with the dataset that gives a different penalty to each misclassification error for the positive class. Specifically, a cost of five is applied to a false negative (marking a bad customer as good) and a cost of one is assigned for a false positive (marking a good customer as bad).</p><ul>\n",
    "<li><strong>Cost for False Negative</strong>: 5</li>\n",
    "<li><strong>Cost for False Positive</strong>: 1</li>\n",
    "</ul><p>This suggests that the positive class is the focus of the prediction task and that it is more costly to the bank or financial institution to give money to a bad customer than to not give money to a good customer. This must be taken into account when selecting a performance metric.</p><p>Next, let’s take a closer look at the data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Explore the Dataset</h2><p>First, download the dataset and save it in your current working directory with the name “<em>german.csv</em>“.</p><ul>\n",
    "<li><a href=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/german.csv\">Download German Credit Dataset (german.csv)</a></li>\n",
    "</ul><p>Review the contents of the file.</p><p>The first few lines of the file should look as follows:<div class=\"crayon-pre\" style=\"font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;\"><div class=\"crayon-line\" id=\"urvanov-syntax-highlighter-5f3f75da15c4c416089650-1\">A11,6,A34,A43,1169,A65,A75,4,A93,A101,4,A121,67,A143,A152,2,A173,1,A192,A201,1</div><div class=\"crayon-line crayon-striped-line\" id=\"urvanov-syntax-highlighter-5f3f75da15c4c416089650-2\">A12,48,A32,A43,5951,A61,A73,2,A92,A101,2,A121,22,A143,A152,1,A173,1,A191,A201,2</div><div class=\"crayon-line\" id=\"urvanov-syntax-highlighter-5f3f75da15c4c416089650-3\">A14,12,A34,A46,2096,A61,A74,2,A93,A101,3,A121,49,A143,A152,1,A172,2,A191,A201,1</div><div class=\"crayon-line crayon-striped-line\" id=\"urvanov-syntax-highlighter-5f3f75da15c4c416089650-4\">A11,42,A32,A42,7882,A61,A74,2,A93,A103,4,A122,45,A143,A153,1,A173,2,A191,A201,1</div><div class=\"crayon-line\" id=\"urvanov-syntax-highlighter-5f3f75da15c4c416089650-5\">A11,24,A33,A40,4870,A61,A73,3,A93,A101,4,A124,53,A143,A153,2,A173,2,A191,A201,2</div><div class=\"crayon-line crayon-striped-line\" id=\"urvanov-syntax-highlighter-5f3f75da15c4c416089650-6\">...</div></div><p>We can see that the categorical columns are encoded with an <em>Axxx</em> format, where “<em>x</em>” are integers for different labels. A one-hot encoding of the categorical variables will be required.</p><p>We can also see that the numerical variables have different scales, e.g. 6, 48, and 12 in column 2, and 1169, 5951, etc. in column 5. This suggests that scaling of the integer columns will be needed for those algorithms that are sensitive to scale.</p><p>The target variable or class is the last column and contains values of 1 and 2. These will need to be label encoded to 0 and 1, respectively, to meet the general expectation for imbalanced binary classification tasks where 0 represents the negative case and 1 represents the positive case.</p><p>The dataset can be loaded as a DataFrame using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\">read_csv() Pandas function</a>, specifying the location and the fact that there is no header line.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding Explained\n",
    "One-hot encoding is a method used to convert categorical data into a format that can be provided to machine learning algorithms to do a better job in prediction. This encoding helps to convert nominal (name-based) categorical data, or categorical ordinal (order-based) data into a numerical format.\n",
    "### How does it work?\n",
    "One-hot encoding works by converting a single column of categorical data into multiple columns of binary values. Each of the new columns represents a unique value from the original column. The new columns (or binary variables) take the value 0 or 1, indicating the absence or presence of some categorical value.\n",
    "### Example:\n",
    "Let's say we have a dataset with a single feature called \"Cars\" with three categories: \"Toyota\", \"Subaru\", and \"Nissan\".\n",
    "### Original Data:\n",
    "|         | **Cars**  |\n",
    "|---------|-----------|\n",
    "|    0    |  Toyota   |\n",
    "|    1    |  Subaru   |\n",
    "|    2    |  Nissan   |        \n",
    "|    3    |  Toyota   |\n",
    "|    4    |  Subaru   |\n",
    "\n",
    "After one-hot encoding, it would be converted to:\n",
    "\n",
    "|         | **Toyota**  | **Subaru**  | **Nissan**  |\n",
    "|---------|-------------|-------------|-------------|\n",
    "|    0    |     1       |      0      |      0      |\n",
    "|    1    |     0       |      1      |      0      |\n",
    "|    2    |     0       |      0      |      1      |        \n",
    "|    3    |     1       |      0      |      0      |\n",
    "|    4    |     0       |      1      |      0      |\n",
    "\n",
    "For the first row of the original data (\"Toyota\"), the \"Toyota\" column in the transformed data gets a 1, while the \"Subaru\" and \"Nissan\" columns get 0s.\n",
    "For the second row of the original data (\"Subaru\"), the \"Subaru\" column gets a 1, while the \"Toyota\" and \"Nissan\" columns get 0s.\n",
    "... and so on.\n",
    "\n",
    "### Caution\n",
    "One-hot encoding can significantly increase the dataset's dimensionality if the categorical variable has many unique values. In such cases, other encoding methods or dimensionality reduction might be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the dataset\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Load the dataset as a DataFrame using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\">read_csv() Pandas function</a>, specifying the location and the fact that there is no header line.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11</td>\n",
       "      <td>6</td>\n",
       "      <td>A34</td>\n",
       "      <td>A43</td>\n",
       "      <td>1169</td>\n",
       "      <td>A65</td>\n",
       "      <td>A75</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>67</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A192</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A12</td>\n",
       "      <td>48</td>\n",
       "      <td>A32</td>\n",
       "      <td>A43</td>\n",
       "      <td>5951</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>2</td>\n",
       "      <td>A92</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>22</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A14</td>\n",
       "      <td>12</td>\n",
       "      <td>A34</td>\n",
       "      <td>A46</td>\n",
       "      <td>2096</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>49</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A172</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A11</td>\n",
       "      <td>42</td>\n",
       "      <td>A32</td>\n",
       "      <td>A42</td>\n",
       "      <td>7882</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A103</td>\n",
       "      <td>...</td>\n",
       "      <td>A122</td>\n",
       "      <td>45</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A11</td>\n",
       "      <td>24</td>\n",
       "      <td>A33</td>\n",
       "      <td>A40</td>\n",
       "      <td>4870</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>3</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A124</td>\n",
       "      <td>53</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1    2    3     4    5    6   7    8     9   ...    11  12    13  \\\n",
       "0  A11   6  A34  A43  1169  A65  A75   4  A93  A101  ...  A121  67  A143   \n",
       "1  A12  48  A32  A43  5951  A61  A73   2  A92  A101  ...  A121  22  A143   \n",
       "2  A14  12  A34  A46  2096  A61  A74   2  A93  A101  ...  A121  49  A143   \n",
       "3  A11  42  A32  A42  7882  A61  A74   2  A93  A103  ...  A122  45  A143   \n",
       "4  A11  24  A33  A40  4870  A61  A73   3  A93  A101  ...  A124  53  A143   \n",
       "\n",
       "     14 15    16 17    18    19 20  \n",
       "0  A152  2  A173  1  A192  A201  1  \n",
       "1  A152  1  A173  1  A191  A201  2  \n",
       "2  A152  1  A172  2  A191  A201  1  \n",
       "3  A153  1  A173  2  A191  A201  1  \n",
       "4  A153  2  A173  2  A191  A201  2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the dataset location\n",
    "filename = 'german.csv'\n",
    "# load the csv file as a data frame\n",
    "dataframe = pd.read_csv(filename, header=None)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_ix = len(dataframe.columns) - 1\n",
    "X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "last_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11</td>\n",
       "      <td>6</td>\n",
       "      <td>A34</td>\n",
       "      <td>A43</td>\n",
       "      <td>1169</td>\n",
       "      <td>A65</td>\n",
       "      <td>A75</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>4</td>\n",
       "      <td>A121</td>\n",
       "      <td>67</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A192</td>\n",
       "      <td>A201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A12</td>\n",
       "      <td>48</td>\n",
       "      <td>A32</td>\n",
       "      <td>A43</td>\n",
       "      <td>5951</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>2</td>\n",
       "      <td>A92</td>\n",
       "      <td>A101</td>\n",
       "      <td>2</td>\n",
       "      <td>A121</td>\n",
       "      <td>22</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A14</td>\n",
       "      <td>12</td>\n",
       "      <td>A34</td>\n",
       "      <td>A46</td>\n",
       "      <td>2096</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>3</td>\n",
       "      <td>A121</td>\n",
       "      <td>49</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A172</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A11</td>\n",
       "      <td>42</td>\n",
       "      <td>A32</td>\n",
       "      <td>A42</td>\n",
       "      <td>7882</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A103</td>\n",
       "      <td>4</td>\n",
       "      <td>A122</td>\n",
       "      <td>45</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A11</td>\n",
       "      <td>24</td>\n",
       "      <td>A33</td>\n",
       "      <td>A40</td>\n",
       "      <td>4870</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>3</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>4</td>\n",
       "      <td>A124</td>\n",
       "      <td>53</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1    2    3     4    5    6   7    8     9   10    11  12    13    14  \\\n",
       "0  A11   6  A34  A43  1169  A65  A75   4  A93  A101   4  A121  67  A143  A152   \n",
       "1  A12  48  A32  A43  5951  A61  A73   2  A92  A101   2  A121  22  A143  A152   \n",
       "2  A14  12  A34  A46  2096  A61  A74   2  A93  A101   3  A121  49  A143  A152   \n",
       "3  A11  42  A32  A42  7882  A61  A74   2  A93  A103   4  A122  45  A143  A153   \n",
       "4  A11  24  A33  A40  4870  A61  A73   3  A93  A101   4  A124  53  A143  A153   \n",
       "\n",
       "   15    16  17    18    19  \n",
       "0   2  A173   1  A192  A201  \n",
       "1   1  A173   1  A191  A201  \n",
       "2   1  A172   2  A191  A201  \n",
       "3   1  A173   2  A191  A201  \n",
       "4   2  A173   2  A191  A201  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    1\n",
       "3    1\n",
       "4    2\n",
       "Name: 20, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Once loaded, we can summarize the number of rows and columns by printing the shape of the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\">DataFrame</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize the shape of the dataset\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can also summarize the number of examples in each class using the <a href=\"https://docs.python.org/3/library/collections.html\">Counter</a> object.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=1, Count=700, Percentage=70.00%\n",
      "Class=2, Count=300, Percentage=30.00%\n"
     ]
    }
   ],
   "source": [
    "# summarize the class distribution\n",
    "target = dataframe.values[:,-1]\n",
    "counter = Counter(target)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(target) * 100\n",
    "    print('Class=%d, Count=%d, Percentage=%.2f%%' % (k, v, per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running the example first loads the dataset and confirms the number of rows and columns, that is 1,000 rows and 20 input variables and 1 target variable.</p><p>The class distribution is then summarized, confirming the number of good and bad customers and the percentage of cases in the minority and majority classes.</p><p>We can also take a look at the distribution of the seven numerical input variables by creating a histogram for each.</p><p>First, we can select the columns with numeric variables by calling the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\">select_dtypes() function</a> on the DataFrame. We can then select just those columns from the DataFrame. We would expect there to be seven, plus the numerical class labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11</td>\n",
       "      <td>6</td>\n",
       "      <td>A34</td>\n",
       "      <td>A43</td>\n",
       "      <td>1169</td>\n",
       "      <td>A65</td>\n",
       "      <td>A75</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>67</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A192</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A12</td>\n",
       "      <td>48</td>\n",
       "      <td>A32</td>\n",
       "      <td>A43</td>\n",
       "      <td>5951</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>2</td>\n",
       "      <td>A92</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>22</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A14</td>\n",
       "      <td>12</td>\n",
       "      <td>A34</td>\n",
       "      <td>A46</td>\n",
       "      <td>2096</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>49</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A172</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A11</td>\n",
       "      <td>42</td>\n",
       "      <td>A32</td>\n",
       "      <td>A42</td>\n",
       "      <td>7882</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A103</td>\n",
       "      <td>...</td>\n",
       "      <td>A122</td>\n",
       "      <td>45</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A11</td>\n",
       "      <td>24</td>\n",
       "      <td>A33</td>\n",
       "      <td>A40</td>\n",
       "      <td>4870</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>3</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A124</td>\n",
       "      <td>53</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1    2    3     4    5    6   7    8     9   ...    11  12    13  \\\n",
       "0  A11   6  A34  A43  1169  A65  A75   4  A93  A101  ...  A121  67  A143   \n",
       "1  A12  48  A32  A43  5951  A61  A73   2  A92  A101  ...  A121  22  A143   \n",
       "2  A14  12  A34  A46  2096  A61  A74   2  A93  A101  ...  A121  49  A143   \n",
       "3  A11  42  A32  A42  7882  A61  A74   2  A93  A103  ...  A122  45  A143   \n",
       "4  A11  24  A33  A40  4870  A61  A73   3  A93  A101  ...  A124  53  A143   \n",
       "\n",
       "     14 15    16 17    18    19 20  \n",
       "0  A152  2  A173  1  A192  A201  1  \n",
       "1  A152  1  A173  1  A191  A201  2  \n",
       "2  A152  1  A172  2  A191  A201  1  \n",
       "3  A153  1  A173  2  A191  A201  1  \n",
       "4  A153  2  A173  2  A191  A201  2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the dataset location\n",
    "filename = 'german.csv'\n",
    "# load the csv file as a data frame\n",
    "df = pd.read_csv(filename, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We select the columns with numeric variables by calling the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\">select_dtypes() function</a> on the DataFrame. We then select just those columns from the DataFrame. We would expect there to be seven, plus the numerical class labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>17</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1169</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>5951</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>2096</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>7882</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>4870</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1     4   7   10  12  15  17  20\n",
       "0   6  1169   4   4  67   2   1   1\n",
       "1  48  5951   2   2  22   1   1   2\n",
       "2  12  2096   2   3  49   1   2   1\n",
       "3  42  7882   2   4  45   1   2   1\n",
       "4  24  4870   3   4  53   2   2   2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select columns with numerical data types\n",
    "num_ix = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "# select a subset of the dataframe with the chosen columns\n",
    "subset = df[num_ix]\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can then create histograms of each numeric input variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a histogram plot of each numeric variable\n",
    "from matplotlib import pyplot\n",
    "ax = subset.hist(figsize=(10, 10))\n",
    "# disable axis labels to avoid the clutter\n",
    "for axis in ax.flatten():\n",
    "    axis.set_xticklabels([])\n",
    "    axis.set_yticklabels([])\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running the example creates the figure with one histogram subplot for each of the seven input variables and one class label in the dataset. The title of each subplot indicates the column number in the DataFrame (e.g. zero-offset from 0 to 20).</p><p>We can see many different distributions, some with <a href=\"https://machinelearningmastery.com/continuous-probability-distributions-for-machine-learning/\">Gaussian-like distributions</a>, others with seemingly exponential or discrete distributions.</p><p>Depending on the choice of modeling algorithms, we would expect scaling the distributions to the same range to be useful, and perhaps the use of some power transforms.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have reviewed the dataset, let’s look at developing a test harness for evaluating candidate models.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Test and Baseline Result</h2><p>We will evaluate candidate models using repeated stratified k-fold cross-validation.</p><p>The <a href=\"https://machinelearningmastery.com/k-fold-cross-validation/\">k-fold cross-validation procedure</a> provides a good general estimate of model performance that is not too optimistically biased, at least compared to a single train-test split. We will use k=10, meaning each fold will contain about 1000/10 or 100 examples.</p><p>Stratified means that each fold will contain the same mixture of examples by class, that is about 70 percent to 30 percent good to bad customers. Repeated means that the evaluation process will be performed multiple times to help avoid fluke results and better capture the variance of the chosen model. We will use three repeats.</p><p>This means a single model will be fit and evaluated 10 * 3 or 30 times and the mean and standard deviation of these runs will be reported.</p><p>This can be achieved using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html\">RepeatedStratifiedKFold scikit-learn class</a>.</p><p>We will predict class labels of whether a customer is good or not. Therefore, we need a measure that is appropriate for evaluating the predicted class labels.</p><p>The focus of the task is on the positive class (bad customers). Precision and recall are a good place to start. Maximizing precision will minimize the false positives and maximizing recall will minimize the false negatives in the predictions made by a model.</p><ul>\n",
    "<li>Precision = TruePositives / (TruePositives + FalsePositives)</li>\n",
    "<li>Recall = TruePositives / (TruePositives + FalseNegatives)</li>\n",
    "</ul><p>Using the F-Measure we will calculate the <a href=\"https://www.mathsisfun.com/numbers/harmonic-mean.html\">harmonic mean</a> between precision and recall. This is a good single number that can be used to compare and select a model on this problem. The issue is that false negatives are more damaging than false positives.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>F-Measure = (2 x Precision x Recall) / (Precision + Recall)</li>\n",
    "</ul><p>Remember that false negatives on this dataset are cases of a bad customer being marked as a good customer and being given a loan. False positives are cases of a good customer being marked as a bad customer and not being given a loan.</p><ul>\n",
    "<li><strong>False Negative</strong>: Bad Customer (class 1) predicted as a Good Customer (class 0).</li>\n",
    "<li><strong>False Positive</strong>: Good Customer (class 0) predicted as a Bad Customer (class 1).</li>\n",
    "</ul><p>False negatives are more costly to the bank than false positives.</p><ul>\n",
    "<li>Cost(False Negatives) &gt; Cost(False Positives)</li>\n",
    "</ul><p>Put another way, we are interested in the F-measure that will summarize a model’s ability to minimize misclassification errors for the positive class, but we want to favor models that are better at minimizing false negatives over false positives.</p><p>This can be achieved by using a version of the F-measure that calculates a weighted <a href=\"https://machinelearningmastery.com/arithmetic-geometric-and-harmonic-means-for-machine-learning/\">harmonic mean</a> of precision and recall but favors higher recall scores over precision scores. This is called the <a href=\"https://machinelearningmastery.com/fbeta-measure-for-machine-learning/\">Fbeta-measure</a>, a generalization of F-measure, where “<em>beta</em>” is a parameter that defines the weighting of the two scores.</p><ul>\n",
    "<li>Fbeta-Measure = ((1 + beta^2) x Precision x Recall) / (beta^2 x Precision + Recall)</li>\n",
    "</ul><p>A beta value of 2 will weight more attention on recall than precision and is referred to as the F2-measure.</p><ul>\n",
    "<li>F2-Measure = ((1 + 2^2) x Precision x Recall) / (2^2 x Precision + Recall)</li>\n",
    "</ul><p>We will use this measure to evaluate models on the German credit dataset. This can be achieved using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\">fbeta_score() scikit-learn function</a>.</p><p>We can define a function to load the dataset and split the columns into input and output variables. We will one-hot encode the categorical variables and label encode the target variable. You might recall that a one-hot encoding replaces the categorical variable with one new column for each value of the variable and marks values with a 1 in the column for that value.</p><p>The complete example of loading the German Credit dataset, evaluating a baseline model, and reporting the performance is listed below.</p><p>First, we must split the DataFrame into input and output variables.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test harness and baseline model evaluation for the german credit dataset\n",
    "from collections import Counter\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>First, we must split the DataFrame into input and output variables.</p><p>Next, we need to select all input variables that are categorical, then apply a one-hot encoding and leave the numerical variables untouched.</p><p>This can be achieved using a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\">ColumnTransformer</a> and defining the transform as a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">OneHotEncoder</a> applied only to the column indices for categorical variables.</p><p>We can then label encode the target variable.</p><p>The <em>load_dataset()</em> function below ties all of this together and loads and prepares the dataset for modeling.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset as a numpy array\n",
    "def load_dataset(full_path):\n",
    "    \n",
    "    # load the dataset as a pandas dataframe\n",
    "    dataframe = pd.read_csv(full_path, header=None)\n",
    "    \n",
    "    # split into inputs and outputs\n",
    "    last_ix = len(dataframe.columns) - 1\n",
    "    X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "    \n",
    "    # select categorical features\n",
    "    cat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    \n",
    "    # one hot encode cat features only\n",
    "    ct = ColumnTransformer([('o',OneHotEncoder(),cat_ix)], remainder='passthrough')\n",
    "    X = ct.fit_transform(X)\n",
    "    \n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, we need a function that will evaluate a set of predictions using the <em>fbeta_score()</em> function with <em>beta</em> set to 2.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f2 score\n",
    "def f2(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can then define a function that will evaluate a given model on the dataset and return a list of F2-Measure scores for each fold and repeat.</p>The evaluate_model() function below implements this, taking the dataset and model as arguments and returning the list of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define the model evaluation metric\n",
    "    metric = make_scorer(f2)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_jobs=-1**: This parameter specifies how many CPU cores to use for the computation. The value -1 means to use all available cores. This allows for parallel computation, making the cross-validation process faster, especially when evaluating on multiple splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 61) (1000,) Counter({0: 700, 1: 300})\n"
     ]
    }
   ],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y = load_dataset(full_path)\n",
    "# summarize the loaded dataset\n",
    "print(X.shape, y.shape, Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00,\n",
       "         0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
       "         1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 0.000e+00,\n",
       "         6.000e+00, 1.169e+03, 4.000e+00, 4.000e+00, 6.700e+01, 2.000e+00,\n",
       "         1.000e+00]]),\n",
       " array([0], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1],y[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Finally, we can evaluate a baseline model on the dataset using this test harness.</p><p>A model that predicts the minority class for examples will achieve a maximum recall score and a baseline precision score. This provides a baseline in model performance on this problem by which all other models can be compared.</p><p>This can be achieved using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html\">DummyClassifier</a> class from the scikit-learn library and setting the “<em>strategy</em>” argument to “<em>constant</em>” and the “<em>constant</em>” argument to “<em>1</em>” for the minority class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the reference model\n",
    "model = DummyClassifier(strategy='constant', constant=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Once the model is evaluated, we can report the mean and standard deviation of the F2-Measure scores directly.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F2: 0.682 (0.000)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = evaluate_model(X, y, model)\n",
    "# summarize performance\n",
    "print('Mean F2: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running the example first loads and summarizes the dataset.</p><p>We can see that we have the correct number of rows loaded, and through the one-hot encoding of the categorical input variables, we have increased the number of input variables from 20 to 61. That suggests that the 13 categorical variables were encoded into a total of 54 columns.</p><p>Importantly, we can see that the class labels have the correct mapping to integers with 0 for the majority class and 1 for the minority class, customary for imbalanced binary classification dataset.</p><p>Next, the average of the F2-Measure scores is reported.</p><p>In this case, we can see that the baseline algorithm achieves an F2-Measure of about 0.682. This score provides a lower limit on model skill; any model that achieves an average F2-Measure above about 0.682 has skill, whereas models that achieve a score below this value do not have skill on this dataset.</p><p>Now that we have a test harness and a baseline in performance, we can begin to evaluate some models on this dataset.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluate Models</h2><p>In this section, we will evaluate a suite of different techniques on the dataset using the test harness developed in the previous section.</p><p>The goal is to both demonstrate how to work through the problem systematically and to demonstrate the capability of some techniques designed for imbalanced classification problems.</p><p>The reported performance is good, but not highly optimized (e.g. hyperparameters are not tuned).</p><p><strong>Can you do better? </strong>If you can achieve better F2-Measure performance using the same test harness, I’d love to hear about it.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluate Machine Learning Algorithms</h3><p>Let’s start by evaluating a mixture of probabilistic machine learning models on the dataset.</p><p>It can be a good idea to spot check a suite of different linear and nonlinear algorithms on a dataset to quickly flush out what works well and deserves further attention, and what doesn’t.</p><p>We will evaluate the following machine learning models on the German credit dataset:</p><ul>\n",
    "<li>Logistic Regression (LR)</li>\n",
    "<li>Linear Discriminant Analysis (LDA)</li>\n",
    "<li>Naive Bayes (NB)</li>\n",
    "<li>Gaussian Process Classifier (GPC)</li>\n",
    "<li>Support Vector Machine (SVM)</li>\n",
    "<li>Decision Tree (DT)</li>\n",
    "</ul><p>The complete example of evaluating a suite of machine learning algorithms on the German credit dataset is listed below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot check machine learning algorithms on the german credit dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will one-hot encode the categorical input variables as we did in the previous section, and in this case, we will normalize the numerical input variables. This is best performed using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">MinMaxScaler</a> within each fold of the cross-validation evaluation process.</p><p>An easy way to implement this is to use a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">Pipeline</a> where the first step is a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\">ColumnTransformer</a> that applies a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">OneHotEncoder</a> to just the categorical variables, and a <em>MinMaxScaler</em> to just the numerical input variables. To achieve this, we need a list of the column indices for categorical and numerical input variables.</p><p>We can update the <em>load_dataset()</em> to return the column indexes as well as the input and output elements of the dataset. The updated version of this function is listed below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset as a numpy array\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a pandas dataframe\n",
    "    dataframe = pd.read_csv(full_path, header=None)\n",
    "    # split into inputs and outputs\n",
    "    last_ix = len(dataframe.columns) - 1\n",
    "    X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "    # select categorical and numerical features\n",
    "    cat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    num_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X.values, y, cat_ix, num_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define the model evaluation metric\n",
    "    metric = make_scorer(f2_measure)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We define each model in turn and add them to a list so that we can evaluate them sequentially. The <em>get_models()</em> function below defines the list of models for evaluation, as well as a list of model short names for plotting the results later.</p><p>We can then enumerate the list of models in turn and evaluate each, storing the scores for later evaluation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models to test\n",
    "def get_models():\n",
    "    # list of models and model names\n",
    "    models, names = [], []\n",
    "    # LR\n",
    "    models.append(LogisticRegression(solver='liblinear'))\n",
    "    names.append('LR')\n",
    "    # LDA\n",
    "    models.append(LinearDiscriminantAnalysis())\n",
    "    names.append('LDA')\n",
    "    # NB\n",
    "    models.append(GaussianNB())\n",
    "    names.append('NB')\n",
    "    # GPC\n",
    "    models.append(GaussianProcessClassifier())\n",
    "    names.append('GPC')\n",
    "    # SVM\n",
    "    models.append(SVC(gamma='scale'))\n",
    "    names.append('SVM')\n",
    "    # DT\n",
    "    models.append(DecisionTreeClassifier())\n",
    "    names.append('DT')\n",
    "    return models, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can then call the function (load_dataset) to get the data and the list of categorical and numerical variables.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define models\n",
    "models, names = get_models()\n",
    "# create a results list\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names: ['LR', 'LDA', 'NB', 'GPC', 'SVM', 'DT']\n",
      "Models: [LogisticRegression(solver='liblinear'), LinearDiscriminantAnalysis(), GaussianNB(), GaussianProcessClassifier(), SVC(), DecisionTreeClassifier()]\n"
     ]
    }
   ],
   "source": [
    "print('Names:', (names))\n",
    "print('Models:', (models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['A11', 6, 'A34', 'A43', 1169, 'A65', 'A75', 4, 'A93', 'A101', 4,\n",
       "         'A121', 67, 'A143', 'A152', 2, 'A173', 1, 'A192', 'A201']],\n",
       "       dtype=object),\n",
       " array([0], dtype=int64))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1],y[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype='int64'),\n",
       " Int64Index([1, 4, 7, 10, 12, 15, 17], dtype='int64'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_ix,num_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This can be used to prepare a <em>Pipeline</em> to wrap each model prior to evaluating it.</p><p>First, the <em>ColumnTransformer</em> is defined, which specifies what transform to apply to each type of column, then this is used as the first step in a Pipeline that ends with the specific model that will be fit and evaluated.</p><p>We can summarize the mean F2-Measure for each algorithm; this will help to directly compare algorithms.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LR 0.498 (0.072)\n",
      ">LDA 0.519 (0.072)\n",
      ">NB 0.639 (0.049)\n",
      ">GPC 0.219 (0.061)\n",
      ">SVM 0.436 (0.077)\n",
      ">DT 0.442 (0.089)\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "    # one hot encode categorical, normalize numerical\n",
    "    ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "    # wrap the model i a pipeline\n",
    "    pipeline = Pipeline(steps=[('t',ct),('m',models[i])])\n",
    "    # evaluate the model and store results\n",
    "    scores = evaluate_model(X, y, pipeline)\n",
    "    results.append(scores) \n",
    "    # summarize and store\n",
    "    print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.62937063, 0.42253521, 0.50359712, 0.57432432, 0.52083333,\n",
       "        0.48611111, 0.43165468, 0.43165468, 0.5033557 , 0.42553191,\n",
       "        0.57823129, 0.52447552, 0.4964539 , 0.43478261, 0.48611111,\n",
       "        0.55172414, 0.42553191, 0.47445255, 0.51724138, 0.49295775,\n",
       "        0.48275862, 0.42553191, 0.46428571, 0.37931034, 0.65972222,\n",
       "        0.68627451, 0.48507463, 0.53191489, 0.41666667, 0.4964539 ])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4979641429755651 0.07168600123526696\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = results[0]\n",
    "mean_value = np.mean(data)\n",
    "stdev = np.std(data)\n",
    "print(mean_value, stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>At the end of the run, we will create a separate box and whisker plot for each algorithm’s sample of results.</p><p>These plots will use the same y-axis scale so we can compare the distribution of results directly.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "# Set the figure size (width, height) in inches\n",
    "pyplot.figure(figsize=(15, 8))\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running the example evaluates each algorithm in turn and reports the mean and standard deviation F2-Measure.</p><p><strong>Note</strong>: Your <a href=\"https://machinelearningmastery.com/different-results-each-time-in-machine-learning/\">results may vary</a> given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.</p><p>In this case, we can see that none of the tested models have an F2-measure above the default of predicting the minority class in all cases (0.682). None of the models are skillful. This is surprising, although suggests that perhaps the decision boundary between the two classes is noisy.</p><p>A figure is created showing one box and whisker plot for each algorithm’s sample of results. The box shows the middle 50 percent of the data, the orange line in the middle of each box shows the median of the sample, and the green triangle in each box shows the mean of the sample.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Box Plots\n",
    "<p>Box plots, also known as whisker plots, are a graphical representation of the distribution of a dataset. They provide a visual summary that combines several important quantitative measures, making them especially useful for understanding the central tendency and variability of a dataset as well as identifying potential outliers.</p><p> In Python, the matplotlib and seaborn libraries are popular choices for creating box plots.</p>\n",
    "\n",
    "### Components of a Box Plot:\n",
    "<img src=\"Screenshot 2023-08-27 at 12.23.04.png\" width=400, height=450>\n",
    "\n",
    "1. **Median (Q2/50th Percentile)**: The median is the value that separates the higher half from the lower half of a data sample. It is represented by a line inside the box.\n",
    "\n",
    "2. **First Quartile (Q1/25th Percentile)**: This is the median of the first half of the data (i.e., the data below the median). It's the bottom line of the box.\n",
    "\n",
    "3. **Third Quartile (Q3/75th Percentile)**: This is the median of the second half of the data (i.e., the data above the median). It's the top line of the box.\n",
    "\n",
    "4. **Interquartile Range (IQR)**: It's the range between Q3 and Q1 (IQR = Q3 - Q1). The height of the box represents this range, and it captures the middle 50% of the data.\n",
    "\n",
    "5. **Whiskers**: The \"T\" lines that extend from the box. They can represent a range of data within a particular distance from the IQR or, in some cases, the minimum and maximum data values.\n",
    "\n",
    "6. **Outliers**: Typically represented by dots or other markers outside of the whiskers. Outliers are data points that fall far from the other data points.\n",
    "\n",
    "### Uses of Box Plots:\n",
    "1. **Understanding Distribution**: You can quickly get a sense of the center, spread, and overall range of the data.\n",
    "\n",
    "2. **Detecting Outliers**: Outliers are displayed outside the \"whiskers\" of the box plot, allowing for easy identification.\n",
    "\n",
    "3. **Comparing Distributions**: When you have data from multiple groups or categories, you can use side-by-side box plots to compare their distributions.\n",
    "\n",
    "4. **Visualizing Skewness**: If the data is skewed, the median might be closer to Q1 or Q3, and one whisker might be notably longer than the other.\n",
    "\n",
    "5. **Evaluating Spread and Symmetry**: The IQR and the position of the median can give insights into the variability and symmetry of the data distribution.\n",
    "\n",
    "<p>In conclusion, box plots are a concise way of graphically summarizing the distribution of a dataset. They are particularly useful for comparing distributions across different categories or groups and for identifying outliers.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have some results, let’s see if we can improve them with some undersampling.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluate Undersampling</h3><p>Undersampling is perhaps the least widely used technique when addressing an imbalanced classification task as most of the focus is put on oversampling the majority class with SMOTE (Synthetic Minority Over-sampling Technique).</p><p>Undersampling can help to remove examples from the majority class along the decision boundary that make the problem challenging for classification algorithms.</p><p>In this experiment we will test the following undersampling algorithms:</p><ul>\n",
    "<li>Tomek Links (TL)</li>\n",
    "<li>Edited Nearest Neighbors (ENN)</li>\n",
    "<li>Repeated Edited Nearest Neighbors (RENN)</li>\n",
    "<li>One Sided Selection (OSS)</li>\n",
    "<li>Neighborhood Cleaning Rule (NCR)</li>\n",
    "<li>NearMiss (NM)</li>\n",
    "</ul><p>The Tomek Links and ENN methods select examples from the majority class to delete, whereas OSS and NCR both select examples to keep and examples to delete. NM undersamples by selecting majority samples based on the distance to minority class samples. We will use the balanced version of the logistic regression algorithm to test each undersampling method, to keep things simple.</p><p>The complete example of evaluating logistic regression with different undersampling methods on the German credit dataset is listed below.</p><p>We would expect the undersampling to to result in a lift on skill in logistic regression, ideally above the baseline performance of predicting the minority class in all cases.</p><p>The complete example is listed below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate undersampling with logistic regression on the imbalanced german credit dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will one-hot encode the categorical input variables as we did in the previous section, and in this case, we will normalize the numerical input variables. This is best performed using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">MinMaxScaler</a> within each fold of the cross-validation evaluation process.</p><p>An easy way to implement this is to use a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">Pipeline</a> where the first step is a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\">ColumnTransformer</a> that applies a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">OneHotEncoder</a> to just the categorical variables, and a <em>MinMaxScaler</em> to just the numerical input variables. To achieve this, we need a list of the column indices for categorical and numerical input variables.</p><p>We can update the <em>load_dataset()</em> to return the column indexes as well as the input and output elements of the dataset. The updated version of this function is listed below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset as a numpy array\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a pandas dataframe\n",
    "    dataframe = pd.read_csv(full_path, header=None)\n",
    "    # split into inputs and outputs\n",
    "    last_ix = len(dataframe.columns) - 1\n",
    "    X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "    # select categorical and numerical features\n",
    "    cat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    num_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X.values, y, cat_ix, num_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define the model evaluation metric\n",
    "    metric = make_scorer(f2_measure)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The <em>get_models()</em> function from the previous section can be updated to return a list of undersampling techniques to test with the logistic regression algorithm. We use the implementations of these algorithms from the imbalanced-learn library.</p><p>The updated version of the <em>get_models()</em> function defining the undersampling methods is listed below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define undersampling models to test\n",
    "def get_models():\n",
    "    models, names = [], []\n",
    "    # TL\n",
    "    models.append(TomekLinks())\n",
    "    names.append('TL')\n",
    "    # ENN\n",
    "    models.append(EditedNearestNeighbours())\n",
    "    names.append('ENN')\n",
    "    # RENN\n",
    "    models.append(RepeatedEditedNearestNeighbours())\n",
    "    names.append('RENN')\n",
    "    # OSS\n",
    "    models.append(OneSidedSelection())\n",
    "    names.append('OSS')\n",
    "    # NCR\n",
    "    models.append(NeighbourhoodCleaningRule())\n",
    "    names.append('NCR')\n",
    "    # NM\n",
    "    models.append(NearMiss())\n",
    "    names.append('NM')\n",
    "    return models, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)\n",
    "# define models\n",
    "models, names = get_models()\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names: ['TL', 'ENN', 'RENN', 'OSS', 'NCR', 'NM']\n",
      "Models: [TomekLinks(), EditedNearestNeighbours(), RepeatedEditedNearestNeighbours(), OneSidedSelection(), NeighbourhoodCleaningRule(), NearMiss()]\n"
     ]
    }
   ],
   "source": [
    "print('Names:', (names))\n",
    "print('Models:', (models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">Pipeline</a> provided by scikit-learn does not know about undersampling algorithms. Therefore, we must use the <a href=\"https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.pipeline.Pipeline.html\">Pipeline</a> implementation provided by the <a href=\"https://imbalanced-learn.readthedocs.io/en/stable/\">imbalanced-learn library</a>.</p><p>As in the previous section, the first step of the pipeline will be one hot encoding of categorical variables and normalization of numerical variables, and the final step will be fitting the model. Here, the middle step will be the undersampling technique, correctly applied within the cross-validation evaluation on the training dataset only.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">TL 0.669 (0.057)\n",
      ">ENN 0.706 (0.048)\n",
      ">RENN 0.714 (0.041)\n",
      ">OSS 0.671 (0.056)\n",
      ">NCR 0.693 (0.052)\n",
      ">NM 0.652 (0.050)\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "    # define model to evaluate\n",
    "    model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "    # one hot encode categorical, normalize numerical\n",
    "    ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "    # scale, then undersample, then fit model\n",
    "    pipeline = Pipeline(steps=[('t',ct), ('s', models[i]), ('m',model)])\n",
    "    # evaluate the model and store results\n",
    "    scores = evaluate_model(X, y, pipeline)\n",
    "    results.append(scores)\n",
    "    # summarize and store\n",
    "    print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# plot the results\n",
    "# Set the figure size (width, height) in inches\n",
    "pyplot.figure(figsize=(15, 8))\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running the example evaluates the logistic regression algorithm with five different undersampling techniques.</p><p><strong>Note</strong>: Your <a href=\"https://machinelearningmastery.com/different-results-each-time-in-machine-learning/\">results may vary</a> given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.</p><p>In this case, we can see that three of the six undersampling techniques resulted in an F2-measure that provides an improvement over the baseline of 0.682. Specifically, ENN, RENN and NCR, with repeated edited nearest neighbors resulting in the best performance with an F2-measure of about 0.714.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Box and whisker plots are created for each evaluated undersampling technique, showing that they generally have the same spread.</p><p>It is encouraging to see that for the well performing methods, the boxes spread up around 0.8, and the mean and median for all three methods are are around 0.7. This highlights that the distributions are skewing high and are let down on occasion by a few bad evaluations.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, let’s see how we might use a final model to make predictions on new data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Further Model Improvements</h3><p>This is a new section that provides a minor departure to the above section. Here, we will test specific models that result in a further lift in F2-measure performance and I will update this section as new models are reported/discovered.</p><h4>Improvement #1: InstanceHardnessThreshold</h4><p>An F2-measure of about <strong>0.727</strong>&nbsp;can be achieved using balanced Logistic Regression with <a href=\"https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.InstanceHardnessThreshold.html\">InstanceHardnessThreshold</a> undersampling.</p><p>The complete example is listed below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve performance on the imbalanced german credit dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a numpy array\n",
    "    dataframe = pd.read_csv(full_path, header=None)\n",
    "    # split into inputs and outputs\n",
    "    last_ix = len(dataframe.columns) - 1\n",
    "    X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "    # select categorical and numerical features\n",
    "    cat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    num_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X.values, y, cat_ix, num_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define the model evaluation metric\n",
    "    metric = make_scorer(f2_measure)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724 (0.036)\n"
     ]
    }
   ],
   "source": [
    "# define model to evaluate\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "# define the data sampling\n",
    "sampling = InstanceHardnessThreshold()\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running the example gives the follow results.</p><p></p><p><strong>Note</strong>: Your <a href=\"https://machinelearningmastery.com/different-results-each-time-in-machine-learning/\">results may vary</a> given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Improvement #2: SMOTEENN (SMOTE and Edited Nearest Neighbours.)</h4><p>An F2-measure of about <strong>0.730</strong>&nbsp;can be achieved using LDA with <a href=\"https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.combine.SMOTEENN.html\">SMOTEENN</a>, where the ENN parameter is set to an ENN instance with sampling_strategy set to majority.</p><p>The complete example is listed below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve performance on the imbalanced german credit dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a numpy array\n",
    "    dataframe = pd.read_csv(full_path, header=None)\n",
    "    # split into inputs and outputs\n",
    "    last_ix = len(dataframe.columns) - 1\n",
    "    X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "    # select categorical and numerical features\n",
    "    cat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    num_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X.values, y, cat_ix, num_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define the model evaluation metric\n",
    "    metric = make_scorer(f2_measure)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.730 (0.039)\n"
     ]
    }
   ],
   "source": [
    "# define model to evaluate\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# define the data sampling\n",
    "sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728 (0.044)\n"
     ]
    }
   ],
   "source": [
    "# define model to evaluate\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# wrap the model\n",
    "model = CalibratedClassifierCV(model, method='sigmoid', cv=3)\n",
    "# define the data sampling\n",
    "sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running the example gives the follow results.</p><p></p><p><strong>Note</strong>: Your <a href=\"https://machinelearningmastery.com/different-results-each-time-in-machine-learning/\">results may vary</a> given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Improvement #3: SMOTEENN with StandardScaler and RidgeClassifier</h4><p>An F2-measure of about <strong>0.737</strong> can be achieved with further improvements to the SMOTEENN using a RidgeClassifier instead of LDA and using a StandardScaler for the numeric inputs instead of a MinMaxScaler.</p><p>The complete example is listed below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve performance on the imbalanced german credit dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a numpy array\n",
    "    dataframe = pd.read_csv(full_path, header=None)\n",
    "    # split into inputs and outputs\n",
    "    last_ix = len(dataframe.columns) - 1\n",
    "    X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "    # select categorical and numerical features\n",
    "    cat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    num_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X.values, y, cat_ix, num_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # define the model evaluation metric\n",
    "    metric = make_scorer(f2_measure)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741 (0.031)\n"
     ]
    }
   ],
   "source": [
    "# define model to evaluate\n",
    "model = RidgeClassifier()\n",
    "# define the data sampling\n",
    "sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',StandardScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.732 (0.033)\n"
     ]
    }
   ],
   "source": [
    "# define model to evaluate\n",
    "model = RidgeClassifier(alpha = 1.0, class_weight='balanced')\n",
    "# wrap the model\n",
    "model = CalibratedClassifierCV(model, method='sigmoid', cv=3)\n",
    "# define the data sampling\n",
    "sampling = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',StandardScaler(),num_ix)])\n",
    "# scale, then sample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', sampling), ('m',model)])\n",
    "# evaluate the model and store results\n",
    "scores = evaluate_model(X, y, pipeline)\n",
    "print('%.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Running the example gives the above results.</p><p></p><p><strong>Note</strong>: Your <a href=\"https://machinelearningmastery.com/different-results-each-time-in-machine-learning/\">results may vary</a> given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Make Prediction on New Data</h2><p>Given the variance in results, a selection of any of the undersampling methods is probably sufficient. In this case, we will select logistic regression with Repeated ENN.</p><p>This model had an F2-measure of about about 0.714 on our test harness.</p><p>We will use this as our final model and use it to make predictions on new data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model and make predictions for the german credit dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a numpy array\n",
    "    dataframe = pd.read_csv(full_path, header=None)\n",
    "    # split into inputs and outputs\n",
    "    last_ix = len(dataframe.columns) - 1\n",
    "    X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "    # select categorical and numerical features\n",
    "    cat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    num_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X.values, y, cat_ix, num_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We define the model as a pipeline.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;t&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;c&#x27;, OneHotEncoder(),\n",
       "                                                  Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype=&#x27;int64&#x27;)),\n",
       "                                                 (&#x27;n&#x27;, MinMaxScaler(),\n",
       "                                                  Int64Index([1, 4, 7, 10, 12, 15, 17], dtype=&#x27;int64&#x27;))])),\n",
       "                (&#x27;s&#x27;, RepeatedEditedNearestNeighbours()),\n",
       "                (&#x27;m&#x27;,\n",
       "                 LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;t&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;c&#x27;, OneHotEncoder(),\n",
       "                                                  Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype=&#x27;int64&#x27;)),\n",
       "                                                 (&#x27;n&#x27;, MinMaxScaler(),\n",
       "                                                  Int64Index([1, 4, 7, 10, 12, 15, 17], dtype=&#x27;int64&#x27;))])),\n",
       "                (&#x27;s&#x27;, RepeatedEditedNearestNeighbours()),\n",
       "                (&#x27;m&#x27;,\n",
       "                 LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                    solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">t: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;c&#x27;, OneHotEncoder(),\n",
       "                                 Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype=&#x27;int64&#x27;)),\n",
       "                                (&#x27;n&#x27;, MinMaxScaler(),\n",
       "                                 Int64Index([1, 4, 7, 10, 12, 15, 17], dtype=&#x27;int64&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">c</label><div class=\"sk-toggleable__content\"><pre>Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype=&#x27;int64&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">n</label><div class=\"sk-toggleable__content\"><pre>Int64Index([1, 4, 7, 10, 12, 15, 17], dtype=&#x27;int64&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RepeatedEditedNearestNeighbours</label><div class=\"sk-toggleable__content\"><pre>RepeatedEditedNearestNeighbours()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('t',\n",
       "                 ColumnTransformer(transformers=[('c', OneHotEncoder(),\n",
       "                                                  Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype='int64')),\n",
       "                                                 ('n', MinMaxScaler(),\n",
       "                                                  Int64Index([1, 4, 7, 10, 12, 15, 17], dtype='int64'))])),\n",
       "                ('s', RepeatedEditedNearestNeighbours()),\n",
       "                ('m',\n",
       "                 LogisticRegression(class_weight='balanced',\n",
       "                                    solver='liblinear'))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model to evaluate\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "# one hot encode categorical, normalize numerical\n",
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "# scale, then undersample, then fit model\n",
    "pipeline = Pipeline(steps=[('t',ct), ('s', RepeatedEditedNearestNeighbours()), ('m',model)])\n",
    "# fit the model\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Once fit, we can use it to make predictions for new data by calling the <em>predict()</em> function. This will return the class label of 0 for “<em>good customer</em>”, or 1 for “<em>bad customer</em>”.</p><p>Importantly, we must use the <em>ColumnTransformer</em> that was fit on the training dataset in the <em>Pipeline</em> to correctly prepare new data using the same transforms.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Customers:\n",
      ">Predicted=0 (expected 0)\n",
      "Bad Customers:\n",
      ">Predicted=1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "# evaluate on some good customers cases (known class 0)\n",
    "print('Good Customers:')\n",
    "data = [['A11', 6, 'A34', 'A43', 1169, 'A65', 'A75', 4, 'A93', 'A101', 4, 'A121', 67, 'A143', 'A152', 2, 'A173', 1, 'A192', 'A201']]\n",
    "for row in data:\n",
    "    # make prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    # get the label\n",
    "    label = yhat[0]#predictions\n",
    "    # summarize\n",
    "    print('>Predicted=%d (expected 0)' % (label))\n",
    "# evaluate on some bad customers (known class 1)\n",
    "print('Bad Customers:')\n",
    "data = [['A13', 18, 'A32', 'A43', 2100, 'A61', 'A73', 4, 'A93', 'A102', 2, 'A121', 37, 'A142', 'A152', 1, 'A173', 1, 'A191', 'A201']]\n",
    "for row in data:\n",
    "    # make prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    # get the label\n",
    "    label = yhat[0]\n",
    "    # summarize\n",
    "    print('>Predicted=%d (expected 1)' % (label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Customers:\n",
      ">Predicted=0 (expected 0)\n",
      ">Predicted=0 (expected 0)\n",
      ">Predicted=1 (expected 0)\n",
      "Bad Customers:\n",
      ">Predicted=1 (expected 1)\n",
      ">Predicted=1 (expected 1)\n",
      ">Predicted=1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "# evaluate on some good customers cases (known class 0)\n",
    "print('Good Customers:')\n",
    "data = [['A11', 6, 'A34', 'A43', 1169, 'A65', 'A75', 4, 'A93', 'A101', 4, 'A121', 67, 'A143', 'A152', 2, 'A173', 1, 'A192', 'A201'],\n",
    "    ['A14', 12, 'A34', 'A46', 2096, 'A61', 'A74', 2, 'A93', 'A101', 3, 'A121', 49, 'A143', 'A152', 1, 'A172', 2, 'A191', 'A201'],\n",
    "    ['A11', 42, 'A32', 'A42', 7882, 'A61', 'A74', 2, 'A93', 'A103', 4, 'A122', 45, 'A143', 'A153', 1, 'A173', 2, 'A191', 'A201']]\n",
    "for row in data:\n",
    "    # make prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    # get the label\n",
    "    label = yhat[0]\n",
    "    # summarize\n",
    "    print('>Predicted=%d (expected 0)' % (label))\n",
    "# evaluate on some bad customers (known class 1)\n",
    "print('Bad Customers:')\n",
    "data = [['A13', 18, 'A32', 'A43', 2100, 'A61', 'A73', 4, 'A93', 'A102', 2, 'A121', 37, 'A142', 'A152', 1, 'A173', 1, 'A191', 'A201'],\n",
    "    ['A11', 24, 'A33', 'A40', 4870, 'A61', 'A73', 3, 'A93', 'A101', 4, 'A124', 53, 'A143', 'A153', 2, 'A173', 2, 'A191', 'A201'],\n",
    "    ['A11', 24, 'A32', 'A43', 1282, 'A62', 'A73', 4, 'A92', 'A101', 2, 'A123', 32, 'A143', 'A152', 1, 'A172', 1, 'A191', 'A201']]\n",
    "for row in data:\n",
    "    # make prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    # get the label\n",
    "    label = yhat[0]\n",
    "    # summarize\n",
    "    print('>Predicted=%d (expected 1)' % (label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "Number of cases where expected is 1 and predicted is 0: 0\n"
     ]
    }
   ],
   "source": [
    "last_ix = len(dataframe.columns) - 1\n",
    "data = dataframe.drop(last_ix, axis=1).values[:50]\n",
    "y = dataframe[last_ix]\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Counter for cases where expected is 1 and predicted is 0\n",
    "counter = 0\n",
    "# Iterate through the data and make predictions,i for labels and row is for features\n",
    "for i, row in enumerate(data):\n",
    "    # Make a prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    \n",
    "    # Get the predicted and actual labels\n",
    "    predicted_label = yhat[0]\n",
    "    actual_label = y[i]\n",
    "    \n",
    "    # Print the results\n",
    "    print(f'> Predicted={predicted_label} (expected={actual_label})')\n",
    "    \n",
    "    # Check if expected is 1 and predicted is 0\n",
    "    if actual_label == 1 and predicted_label == 0:\n",
    "        counter += 1\n",
    "\n",
    "print(f'Number of cases where expected is 1 and predicted is 0: {counter}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[415 285]\n",
      " [ 36 264]]\n",
      "False Negatives: 36\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics, preprocessing\n",
    "yhat = pipeline.predict(X)\n",
    "cm = metrics.confusion_matrix(y, yhat)\n",
    "print(cm)\n",
    "# Extract and print the False Negative count\n",
    "false_negatives = cm[1, 0]\n",
    "print(f\"False Negatives: {false_negatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German dataset cost matrix\n",
    "\n",
    "~~~~\n",
    "                               Actual \"+\"(0)       Actual  \"-\"(1)\n",
    "\n",
    "      Model says: \"+\"(0)            0          |        1\n",
    "                             -------------------------------------\n",
    "      Model says: \"-\"(1)            5          |        0\n",
    "  \n",
    "~~~~\n",
    "\n",
    "Misclassification cost of this model => (5 x 36) + (285 x 1) = 465"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Serialization\n",
    "Serialization of a machine learning (ML) model refers to the process of converting the model's internal state and structure into a format that can be easily stored and later reconstructed. In other words, serialization allows you to save a trained ML model to a file so that you can load and use it again in the future without having to retrain it from scratch. This is particularly useful for sharing, deploying, or archiving trained models.\n",
    "\n",
    "When a model is serialized, its parameters, coefficients, hyperparameters, and any other relevant information are saved in a structured format that preserves the model's structure and learned patterns. The serialization process ensures that the model can be accurately reconstructed when it's deserialized (loaded) later.\n",
    "\n",
    "Serialization is especially important when you want to:\n",
    "\n",
    "1. **Share Models**: You can share your trained model with others, allowing them to use it without needing to access your original training data and code.\n",
    "\n",
    "2. **Deploy Models**: When deploying ML models in production environments, you can serialize the trained model and load it on demand for making predictions.\n",
    "\n",
    "3. **Archive Models**: Saving serialized models ensures that you can replicate results or continue using specific models even if the original training data or code is no longer available.\n",
    "\n",
    "4. **Caching**: Serializing models can be useful for caching predictions in scenarios where making predictions is computationally expensive. You can save predictions alongside their inputs and reload the model to avoid repeated computations.\n",
    "\n",
    "Common serialization formats include:\n",
    "\n",
    "1. **Pickle (Python)**: Python's pickle module is a built-in way to serialize Python objects, including ML models. However, it's important to note that pickle might not be the best choice for large models or for sharing models across different Python versions.\n",
    "\n",
    "2. **Joblib**: The joblib library, often used in conjunction with scikit-learn, is optimized for efficiently saving and loading large numpy arrays, which are common in machine learning models.\n",
    "\n",
    "3. **ONNX (Open Neural Network Exchange)**: ONNX is an open format for representing machine learning models. It enables interoperability between different frameworks and platforms.\n",
    "\n",
    "4. **HDF5 (Hierarchical Data Format)**: HDF5 is a versatile format that can store complex datasets, including machine learning models.\n",
    "\n",
    "Serialization is a crucial part of the machine learning workflow, enabling the transfer of models from development to production and facilitating collaboration and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving final model to disk using Pickle\n",
    "Import **pickle** module, which provides functionality for serializing (pickling) and deserializing (unpickling) Python objects.\n",
    "### Serializing and Saving the Model to Disk:\n",
    "We use the **pickle.dump()** function to serialize the **model** object and save it to a file named **LR_RENN.pkl**. Here's what each argument means:\n",
    "\n",
    "- **model**: The object you want to serialize and save.\n",
    "- **open('LR_RENN.pkl', 'wb')**: Opens the file **LR_RENN.pkl** in binary write mode **('wb')** for writing serialized data into it.\n",
    "\n",
    "### Saving the Pipeline to Disk:\n",
    "Similar to the model, the **pipeline** object is also serialized using **pickle.dump()** and saved to a file named **pipeline.pkl**.\n",
    "\n",
    "This is how to save a trained LogisticRegression model and/or a pipeline containing data preprocessing steps and the model to separate files using the pickle module. This allows you to later load and reuse the trained model and pipeline without having to retrain them. It's important to note that while pickle is a convenient way to save models, it may not always be the most secure or efficient method, particularly in situations where compatibility between different Python versions or security are concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model to disk\n",
    "import pickle\n",
    "# Creating and Configuring the Model\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "# Saving Model to Disk using pickle\n",
    "pickle.dump(model,open('LR_RENN.pkl','wb'))\n",
    "# Save pipeline to disk\n",
    "pickle.dump(pipeline, open('pipeline.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving final model to disk using joblib library\n",
    "You can achieve the same result using the joblib library, which is often preferred for serializing scikit-learn models due to its improved performance on large numpy arrays. Here's how you can save the models to disk using joblib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline.joblib']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# Creating and Configuring the Model\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "# Saving Model to Disk using joblib\n",
    "joblib.dump(model, 'LR_RENN.joblib')\n",
    "# Save pipeline to disk\n",
    "joblib.dump(pipeline, 'pipeline.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import TomekLinks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a numpy array\n",
    "    dataframe = pd.read_csv(full_path, header=None)\n",
    "    # split into inputs and outputs\n",
    "    last_ix = len(dataframe.columns) - 1\n",
    "    X, y = dataframe.drop(last_ix, axis=1), dataframe[last_ix]\n",
    "    # select categorical and numerical features\n",
    "    cat_ix = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    num_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X.values, y, cat_ix, num_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the location of the dataset\n",
    "full_path = 'german.csv'\n",
    "# load the dataset\n",
    "X, y, cat_ix, num_ix = load_dataset(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;t&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;c&#x27;,\n",
       "                                                  OneHotEncoder(drop=&#x27;first&#x27;),\n",
       "                                                  Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype=&#x27;int64&#x27;)),\n",
       "                                                 (&#x27;n&#x27;, MinMaxScaler(),\n",
       "                                                  Int64Index([1, 4, 7, 10, 12, 15, 17], dtype=&#x27;int64&#x27;))])),\n",
       "                (&#x27;s&#x27;, TomekLinks(sampling_strategy=&#x27;majority&#x27;)),\n",
       "                (&#x27;m&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;t&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;c&#x27;,\n",
       "                                                  OneHotEncoder(drop=&#x27;first&#x27;),\n",
       "                                                  Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype=&#x27;int64&#x27;)),\n",
       "                                                 (&#x27;n&#x27;, MinMaxScaler(),\n",
       "                                                  Int64Index([1, 4, 7, 10, 12, 15, 17], dtype=&#x27;int64&#x27;))])),\n",
       "                (&#x27;s&#x27;, TomekLinks(sampling_strategy=&#x27;majority&#x27;)),\n",
       "                (&#x27;m&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">t: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;c&#x27;, OneHotEncoder(drop=&#x27;first&#x27;),\n",
       "                                 Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype=&#x27;int64&#x27;)),\n",
       "                                (&#x27;n&#x27;, MinMaxScaler(),\n",
       "                                 Int64Index([1, 4, 7, 10, 12, 15, 17], dtype=&#x27;int64&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">c</label><div class=\"sk-toggleable__content\"><pre>Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype=&#x27;int64&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(drop=&#x27;first&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">n</label><div class=\"sk-toggleable__content\"><pre>Int64Index([1, 4, 7, 10, 12, 15, 17], dtype=&#x27;int64&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TomekLinks</label><div class=\"sk-toggleable__content\"><pre>TomekLinks(sampling_strategy=&#x27;majority&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('t',\n",
       "                 ColumnTransformer(transformers=[('c',\n",
       "                                                  OneHotEncoder(drop='first'),\n",
       "                                                  Int64Index([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19], dtype='int64')),\n",
       "                                                 ('n', MinMaxScaler(),\n",
       "                                                  Int64Index([1, 4, 7, 10, 12, 15, 17], dtype='int64'))])),\n",
       "                ('s', TomekLinks(sampling_strategy='majority')),\n",
       "                ('m', RandomForestClassifier())])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the new model\n",
    "model = RandomForestClassifier(n_estimators=100)  # Change to your desired model\n",
    "\n",
    "# Define the new preprocessing steps, which may include one-hot encoding and normalization\n",
    "ct = ColumnTransformer([\n",
    "    ('c', OneHotEncoder(drop='first'), cat_ix),\n",
    "    ('n', MinMaxScaler(), num_ix)\n",
    "])\n",
    "\n",
    "# Define the new undersampling technique\n",
    "undersampler = TomekLinks(sampling_strategy='majority')  # Change to your desired undersampling technique\n",
    "\n",
    "# Create the new pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('t', ct),\n",
    "    ('s', undersampler),\n",
    "    ('m', model)\n",
    "])\n",
    "\n",
    "# Fit the new model\n",
    "pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Customers:\n",
      "> Predicted=0 (expected 0)\n",
      "Bad Customers:\n",
      "> Predicted=1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on some good customers cases (known class 0)\n",
    "print('Good Customers:')\n",
    "data = [['A11', 6, 'A34', 'A43', 1169, 'A65', 'A75', 4, 'A93', 'A101', 4, 'A121', 67, 'A143', 'A152', 2, 'A173', 1, 'A192', 'A201']]\n",
    "for row in data:\n",
    "    # Make prediction using the pipeline\n",
    "    yhat = pipeline.predict([row])\n",
    "    \n",
    "    # Get the predicted label\n",
    "    label = yhat[0]\n",
    "    \n",
    "    # Summarize the prediction\n",
    "    print('> Predicted=%d (expected 0)' % (label))\n",
    "\n",
    "# Evaluate on some bad customers (known class 1)\n",
    "print('Bad Customers:')\n",
    "data = [['A13', 18, 'A32', 'A43', 2100, 'A61', 'A73', 4, 'A93', 'A102', 2, 'A121', 37, 'A142', 'A152', 1, 'A173', 1, 'A191', 'A201']]\n",
    "for row in data:\n",
    "    # Make prediction using the pipeline\n",
    "    yhat = pipeline.predict([row])\n",
    "    \n",
    "    # Get the predicted label\n",
    "    label = yhat[0]\n",
    "    \n",
    "    # Summarize the prediction\n",
    "    print('> Predicted=%d (expected 1)' % (label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Customers:\n",
      ">Predicted=0 (expected 0)\n",
      ">Predicted=0 (expected 0)\n",
      ">Predicted=0 (expected 0)\n",
      "Bad Customers:\n",
      ">Predicted=1 (expected 1)\n",
      ">Predicted=1 (expected 1)\n",
      ">Predicted=1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "# evaluate on some good customers cases (known class 0)\n",
    "print('Good Customers:')\n",
    "data = [['A11', 6, 'A34', 'A43', 1169, 'A65', 'A75', 4, 'A93', 'A101', 4, 'A121', 67, 'A143', 'A152', 2, 'A173', 1, 'A192', 'A201'],\n",
    "    ['A14', 12, 'A34', 'A46', 2096, 'A61', 'A74', 2, 'A93', 'A101', 3, 'A121', 49, 'A143', 'A152', 1, 'A172', 2, 'A191', 'A201'],\n",
    "    ['A11', 42, 'A32', 'A42', 7882, 'A61', 'A74', 2, 'A93', 'A103', 4, 'A122', 45, 'A143', 'A153', 1, 'A173', 2, 'A191', 'A201']]\n",
    "for row in data:\n",
    "    # make prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    # get the label\n",
    "    label = yhat[0]\n",
    "    # summarize\n",
    "    print('>Predicted=%d (expected 0)' % (label))\n",
    "# evaluate on some bad customers (known class 1)\n",
    "print('Bad Customers:')\n",
    "data = [['A13', 18, 'A32', 'A43', 2100, 'A61', 'A73', 4, 'A93', 'A102', 2, 'A121', 37, 'A142', 'A152', 1, 'A173', 1, 'A191', 'A201'],\n",
    "    ['A11', 24, 'A33', 'A40', 4870, 'A61', 'A73', 3, 'A93', 'A101', 4, 'A124', 53, 'A143', 'A153', 2, 'A173', 2, 'A191', 'A201'],\n",
    "    ['A11', 24, 'A32', 'A43', 1282, 'A62', 'A73', 4, 'A92', 'A101', 2, 'A123', 32, 'A143', 'A152', 1, 'A172', 1, 'A191', 'A201']]\n",
    "for row in data:\n",
    "    # make prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    # get the label\n",
    "    label = yhat[0]\n",
    "    # summarize\n",
    "    print('>Predicted=%d (expected 1)' % (label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=1 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=1 (expected=1)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "> Predicted=0 (expected=0)\n",
      "Number of cases where expected is 1 and predicted is 0: 0\n"
     ]
    }
   ],
   "source": [
    "last_ix = len(dataframe.columns) - 1\n",
    "data = dataframe.drop(last_ix, axis=1).values[:50]\n",
    "y = dataframe[last_ix]\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Counter for cases where expected is 1 and predicted is 0\n",
    "counter = 0\n",
    "# Iterate through the data and make predictions,i for labels and row is for features\n",
    "for i, row in enumerate(data):\n",
    "    # Make a prediction\n",
    "    yhat = pipeline.predict([row])\n",
    "    \n",
    "    # Get the predicted and actual labels\n",
    "    predicted_label = yhat[0]\n",
    "    actual_label = y[i]\n",
    "    \n",
    "    # Print the results\n",
    "    print(f'> Predicted={predicted_label} (expected={actual_label})')\n",
    "    \n",
    "    # Check if expected is 1 and predicted is 0\n",
    "    if actual_label == 1 and predicted_label == 0:\n",
    "        counter += 1\n",
    "\n",
    "print(f'Number of cases where expected is 1 and predicted is 0: {counter}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[688  12]\n",
      " [  0 300]]\n",
      "False Negatives: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics, preprocessing\n",
    "yhat = pipeline.predict(X)\n",
    "cm = metrics.confusion_matrix(y, yhat)\n",
    "print(cm)\n",
    "# Extract and print the False Negative count\n",
    "false_negatives = cm[1, 0]\n",
    "print(f\"False Negatives: {false_negatives}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
