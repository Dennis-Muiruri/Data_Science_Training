{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Implementing Linear Regression From Scratch in Python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The core of many machine learning algorithms is optimization.</p><p>Optimization algorithms are used by machine learning algorithms to find a good set of model parameters given a training dataset.</p><p>The most common optimization algorithm used in machine learning is stochastic gradient descent.</p><p>In this notebook, you will discover how to implement stochastic gradient descent to optimize a linear regression algorithm from scratch with Python.</p><p>After completing this tutorial, you will know:</p><ul>\n",
    "<li>How to estimate linear regression coefficients using stochastic gradient descent.</li>\n",
    "<li>How to make predictions for multivariate linear regression.</li>\n",
    "<li>How to implement linear regression with stochastic gradient descent to make predictions on new data.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Description</h2><p>In this section, we will describe linear regression, the stochastic gradient descent technique and the wine quality dataset used in this tutorial.</p><h3>Multivariate Linear Regression</h3><p>Linear regression is a technique for predicting a real value.</p><p>Confusingly, these problems where a real value is to be predicted are called regression problems.</p><p>Linear regression is a technique where a straight line is used to model the relationship between input and output values. In more than two dimensions, this straight line may be thought of as a plane or hyperplane.</p><p>Predictions are made as a combination of the input values to predict the output value.</p><p>Each input attribute (x) is weighted using a coefficient (b), and the goal of the learning algorithm is to discover a set of coefficients that results in good predictions (y).</p><p></p><p>$$ y = b_0 + b_1 x_1 + b_2 x_2 + \\dots $$</p><p>Coefficients can be found using stochastic gradient descent.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stochastic Gradient Descent</h3><p>Gradient Descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest decrease of that function.</p><p>In the context of machine learning, we often have a \"cost\" or \"loss\" function that measures how wrong our model is in terms of its ability to estimate the relationship between X (inputs) and y (outputs). The cost function measures how \"costly\" our current set of model coefficients are, in terms of getting predictions wrong when we use them. In linear regression, the cost function could be the Mean Squared Error (MSE). Minimizing this function is the same as making our predictions as accurate as possible.</p><p>The \"gradient\" captures the sensitivity or rate of change of the cost function concerning slight changes in each of its coefficients. The gradient always points in the direction of steepest increase of the function. Thus, the negative gradient points in the direction of steepest decrease. In simpler terms, we're \"descending\" down the cost function's curve by going in the direction where it drops the steepest, until we hopefully reach a minimum point where the cost is as low as it can get given our model and data.</p>\n",
    "<img src=\"rmse_cost_function_curve_example.png\" width=400, height=450>\n",
    "<p>In machine learning, we can use a technique that evaluates and updates the coefficients every iteration called stochastic gradient descent to minimize the error of a model on our training data.</p><p>The way this optimization algorithm works is that each training instance is shown to the model one at a time. The model makes a prediction for a training instance, the error is calculated and the model is updated in order to reduce the error for the next prediction. This process is repeated for a fixed number of iterations.</p><p>This procedure can be used to find the set of coefficients in a model that result in the smallest error for the model on the training data. Each iteration, the coefficients (b) in machine learning language are updated using the equation:</p><p></p><p>$$ b = b - \\text{learning_rate} \\times \\text{error} \\times x $$</p><p>Where <strong>b</strong> is the coefficient or weight being optimized, <strong>learning_rate</strong> is a learning rate that you must configure (e.g. 0.01), <strong>error</strong> is the prediction error for the model on the training data attributed to the weight, and <strong>x</strong> is the input value.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Wine Quality Dataset</h3><p>After we develop our linear regression algorithm with stochastic gradient descent, we will use it to model the wine quality dataset.</p><p>This dataset is comprised of the details of 4,898 white wines including measurements like acidity and pH. The goal is to use&nbsp;these objective measures to predict the wine quality on a scale between 0 and 10.</p><p>Below is a sample of the first 5 records from this dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3      4     5      6       7     8     9     10  11\n",
       "0  7.0  0.27  0.36  20.7  0.045  45.0  170.0  1.0010  3.00  0.45   8.8   6\n",
       "1  6.3  0.30  0.34   1.6  0.049  14.0  132.0  0.9940  3.30  0.49   9.5   6\n",
       "2  8.1  0.28  0.40   6.9  0.050  30.0   97.0  0.9951  3.26  0.44  10.1   6\n",
       "3  7.2  0.23  0.32   8.5  0.058  47.0  186.0  0.9956  3.19  0.40   9.9   6\n",
       "4  7.2  0.23  0.32   8.5  0.058  47.0  186.0  0.9956  3.19  0.40   9.9   6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('winequality-white.csv', header = None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       4898 non-null   float64\n",
      " 1   1       4898 non-null   float64\n",
      " 2   2       4898 non-null   float64\n",
      " 3   3       4898 non-null   float64\n",
      " 4   4       4898 non-null   float64\n",
      " 5   5       4898 non-null   float64\n",
      " 6   6       4898 non-null   float64\n",
      " 7   7       4898 non-null   float64\n",
      " 8   8       4898 non-null   float64\n",
      " 9   9       4898 non-null   float64\n",
      " 10  10      4898 non-null   float64\n",
      " 11  11      4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The dataset must be normalized to the values between 0 and 1 as each attribute has&nbsp;different units and in turn different scales.</p><p>By predicting the mean value (Zero Rule Algorithm) on the normalized dataset, a baseline root mean squared error (RMSE) of 0.148 can be achieved.</p><p>You can learn more about the dataset on the <a href=\"http://archive.ics.uci.edu/ml/datasets/Wine+Quality\">UCI Machine Learning Repository</a>.</p><p>You can download the dataset and save it in your current working directory with the name <strong>winequality-white.csv</strong>. You must remove the header information from the start of the file, and convert the “;” value separator to “,” to meet CSV format.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Notebook</h2><p>This notebook is broken down into 3&nbsp;parts:</p><ol>\n",
    "<li>Making Predictions.</li>\n",
    "<li>Estimating Coefficients.</li>\n",
    "<li>Wine Quality Prediction.</li>\n",
    "</ol><p>This will provide the foundation you need to implement and apply linear regression with stochastic gradient descent on your own predictive modeling problems.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Making Predictions</h3><p>The first step is to develop a function that can make predictions.</p><p>This will be needed both in the evaluation of candidate coefficient values in stochastic gradient descent and after the model is finalized and we wish to start making predictions on test data or new data.</p><p>Below is a function named <strong>predict()</strong> that predicts an output value for a row given a set of coefficients.</p><p>The first coefficient in is always the intercept, also called the bias or b0 as it is standalone and not responsible for a specific input value.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can contrive a small dataset to test our prediction function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  1\n",
       "1  2  3\n",
       "2  4  3\n",
       "3  3  2\n",
       "4  5  5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('test.csv', header = None)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Below is a scatter plot of this dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "plt = df2.plot.scatter(x = 0, y= 1, title ='Small Contrived Dataset For Linear Regression', figsize=(8,4),marker='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can also use previously prepared coefficients to make predictions for this dataset.</p><p>Putting this all together we can test our <strong>predict()</strong> function below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=1.000, Predicted=1.200\n",
      "Expected=3.000, Predicted=2.000\n",
      "Expected=3.000, Predicted=3.600\n",
      "Expected=2.000, Predicted=2.800\n",
      "Expected=5.000, Predicted=4.400\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return yhat\n",
    " \n",
    "#dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "dataset = df2.values\n",
    "coef = [0.4, 0.8]\n",
    "for row in dataset:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>There is a single input value (x) and two coefficient values (b0 and b1). The prediction equation we have modeled for this problem is:</p><p></p><p>$$ y = b_0 + b_1 x_1 $$</p><p>or, with the specific coefficient values we chose by hand as:</p><p></p><p>$$ y = 0.4 + 0.8 x_1 $$</p><p>Now we are ready to implement stochastic gradient descent to optimize our coefficient values.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Estimating Coefficients</h3><p>We can estimate the coefficient values for our training data using stochastic gradient descent.</p><p>Stochastic gradient descent requires two parameters:</p><ul>\n",
    "<li><strong>Learning Rate</strong>: Used to limit the amount each coefficient is corrected each time it is updated.</li>\n",
    "<li><strong>Epochs</strong>: The number of times to run through the training data while updating the coefficients.</li>\n",
    "</ul><p>These, along with the training data will be the arguments to the function.</p><p>There are 3 loops we need to perform in the function:</p><ol>\n",
    "<li>Loop over each epoch.</li>\n",
    "<li>Loop over each row in the training data for an epoch.</li>\n",
    "<li>Loop over each coefficient and update it for a row in an epoch.</li>\n",
    "</ol><p>As you can see, we update each coefficient for each row in the training data, each epoch.</p><p>Coefficients are updated based on the error the model made. The error is calculated as the difference between the prediction made with the candidate coefficients and the expected output value.</p><p></p><p>$$ \\text{error} = \\text{prediction} - \\text{expected} $$\n",
    "</p><p>There is one coefficient to weight each input attribute, and these are updated in a consistent way, for example:</p><p></p><p>$$ b_1(t+1) = b_1(t) - \\text{learning_rate} \\times \\text{error}(t) \\times x_1(t) $$\n",
    "</p><p>The special coefficient at the beginning of the list, also called the intercept or the bias, is updated in a similar way, except without an input as it is not associated with a specific input value:</p><p></p><p>$$ b_0(t+1) = b_0(t) - \\text{learning_rate} \\times \\text{error}(t) $$\n",
    "</p><p>Now we can put all of this together. Below is a function named <strong>coefficients_sgd()</strong> that calculates coefficient values for a training dataset using stochastic gradient descent.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = yhat - row[-1]\n",
    "            sum_error += error**2\n",
    "            coef[0] = coef[0] - l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] - l_rate * error * row[i]\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You can see, that in addition, we keep track of the sum of the squared error (a positive value) each epoch so that we can print out a nice message in the outer loop.</p><p>We can test this function on the same small contrived dataset from above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.001, error=46.236\n",
      ">epoch=1, lrate=0.001, error=41.305\n",
      ">epoch=2, lrate=0.001, error=36.930\n",
      ">epoch=3, lrate=0.001, error=33.047\n",
      ">epoch=4, lrate=0.001, error=29.601\n",
      ">epoch=5, lrate=0.001, error=26.543\n",
      ">epoch=6, lrate=0.001, error=23.830\n",
      ">epoch=7, lrate=0.001, error=21.422\n",
      ">epoch=8, lrate=0.001, error=19.285\n",
      ">epoch=9, lrate=0.001, error=17.389\n",
      ">epoch=10, lrate=0.001, error=15.706\n",
      ">epoch=11, lrate=0.001, error=14.213\n",
      ">epoch=12, lrate=0.001, error=12.888\n",
      ">epoch=13, lrate=0.001, error=11.712\n",
      ">epoch=14, lrate=0.001, error=10.668\n",
      ">epoch=15, lrate=0.001, error=9.742\n",
      ">epoch=16, lrate=0.001, error=8.921\n",
      ">epoch=17, lrate=0.001, error=8.191\n",
      ">epoch=18, lrate=0.001, error=7.544\n",
      ">epoch=19, lrate=0.001, error=6.970\n",
      ">epoch=20, lrate=0.001, error=6.461\n",
      ">epoch=21, lrate=0.001, error=6.009\n",
      ">epoch=22, lrate=0.001, error=5.607\n",
      ">epoch=23, lrate=0.001, error=5.251\n",
      ">epoch=24, lrate=0.001, error=4.935\n",
      ">epoch=25, lrate=0.001, error=4.655\n",
      ">epoch=26, lrate=0.001, error=4.406\n",
      ">epoch=27, lrate=0.001, error=4.186\n",
      ">epoch=28, lrate=0.001, error=3.990\n",
      ">epoch=29, lrate=0.001, error=3.816\n",
      ">epoch=30, lrate=0.001, error=3.662\n",
      ">epoch=31, lrate=0.001, error=3.525\n",
      ">epoch=32, lrate=0.001, error=3.404\n",
      ">epoch=33, lrate=0.001, error=3.296\n",
      ">epoch=34, lrate=0.001, error=3.200\n",
      ">epoch=35, lrate=0.001, error=3.115\n",
      ">epoch=36, lrate=0.001, error=3.040\n",
      ">epoch=37, lrate=0.001, error=2.973\n",
      ">epoch=38, lrate=0.001, error=2.914\n",
      ">epoch=39, lrate=0.001, error=2.862\n",
      ">epoch=40, lrate=0.001, error=2.815\n",
      ">epoch=41, lrate=0.001, error=2.773\n",
      ">epoch=42, lrate=0.001, error=2.737\n",
      ">epoch=43, lrate=0.001, error=2.704\n",
      ">epoch=44, lrate=0.001, error=2.675\n",
      ">epoch=45, lrate=0.001, error=2.650\n",
      ">epoch=46, lrate=0.001, error=2.627\n",
      ">epoch=47, lrate=0.001, error=2.607\n",
      ">epoch=48, lrate=0.001, error=2.589\n",
      ">epoch=49, lrate=0.001, error=2.573\n",
      "Expected=1.000, Predicted=1.032\n",
      "Expected=3.000, Predicted=1.833\n",
      "Expected=3.000, Predicted=3.437\n",
      "Expected=2.000, Predicted=2.635\n",
      "Expected=5.000, Predicted=4.239\n",
      "[0.22998234937311363, 0.8017220304137576]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return yhat\n",
    " \n",
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = yhat - row[-1]\n",
    "            sum_error += error**2\n",
    "            coef[0] = coef[0] - l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] - l_rate * error * row[i]\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return coef\n",
    " \n",
    "# Calculate coefficients\n",
    "#dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "dataset = df2.values\n",
    "l_rate = 0.001\n",
    "#l_rate = 0.0050\n",
    "n_epoch = 50\n",
    "#n_epoch = 200\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "for row in dataset:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))\n",
    "print(coef)\n",
    "#coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We use a small learning rate of 0.001 and train the model for 50 epochs, or 50 exposures of the coefficients to the entire training dataset.</p><p>Running the example prints a message each epoch with the sum squared error for that epoch and the final set of coefficients.</p><p>You can see how error continues to drop even in the final epoch. We could probably train for a lot longer (more epochs) or increase the amount we update the coefficients each epoch (higher learning rate).</p><p>Experiment and see what you come up with.</p><p>Now, let’s apply this algorithm on a real dataset.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Wine Quality Prediction</h3><p>In this section, we will train a linear regression model using stochastic gradient descent on the wine quality dataset.</p><p>The example assumes that a CSV copy of the dataset is in the current working directory with the filename <strong>winequality-white.csv</strong>.</p><p>The dataset is first loaded, the string values converted to numeric and each column is normalized to values in the range of 0 to 1. This is achieved with helper functions <strong>load_csv()</strong> and <strong>str_column_to_float()</strong> to load and prepare the dataset and <strong>dataset_minmax()</strong> and <strong>normalize_dataset()</strong> to normalize it.</p><p>We will use k-fold cross-validation to estimate the performance of the learned model on unseen data. This means that we will construct and evaluate k models and estimate the performance as the mean model error. Root Mean Squared Error will be used to evaluate each model. These behaviors are provided in the <strong>cross_validation_split()</strong>, <strong>rmse_metric()</strong> and <strong>evaluate_algorithm()</strong> helper functions.</p><p>We will use the <strong>predict()</strong>, <strong>coefficients_sgd()</strong> and <strong>linear_regression_sgd()</strong> functions created above to train the model.</p><p>Below is the complete example.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression With Stochastic Gradient Descent for Wine Quality\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = []\n",
    "    #dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = []\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = []\n",
    "    #dataset_copy = list(dataset)\n",
    "    dataset_copy = dataset\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = []\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = []\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        rmse = rmse_metric(actual, predicted)\n",
    "        scores.append(rmse)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return yhat\n",
    " \n",
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = yhat - row[-1]\n",
    "            coef[0] = coef[0] - l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] - l_rate * error * row[i]\n",
    "            #print(l_rate, n_epoch, error)\n",
    "    return coef\n",
    " \n",
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def linear_regression_sgd(train, test, l_rate, n_epoch):\n",
    "    predictions = []\n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        predictions.append(yhat)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_minmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     str_column_to_float(dataset, i)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# normalize\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m minmax \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_minmax\u001b[49m(dataset)\n\u001b[0;32m     10\u001b[0m normalize_dataset(dataset, minmax)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# evaluate algorithm\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_minmax' is not defined"
     ]
    }
   ],
   "source": [
    "# Linear Regression on wine quality dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'winequality-white.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.001\n",
    "n_epoch = 200\n",
    "scores = evaluate_algorithm(dataset, linear_regression_sgd, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean RMSE: %.3f' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0.500, Predicted=0.402\n",
      "Expected=0.500, Predicted=0.382\n",
      "Expected=0.500, Predicted=0.432\n",
      "Expected=0.500, Predicted=0.444\n",
      "Expected=0.500, Predicted=0.444\n",
      "Expected=0.500, Predicted=0.432\n",
      "Expected=0.500, Predicted=0.400\n",
      "Expected=0.500, Predicted=0.402\n",
      "Expected=0.500, Predicted=0.382\n",
      "Expected=0.500, Predicted=0.485\n",
      "Expected=0.333, Predicted=0.524\n",
      "Expected=0.333, Predicted=0.410\n",
      "Expected=0.333, Predicted=0.491\n",
      "Expected=0.667, Predicted=0.618\n",
      "Expected=0.333, Predicted=0.417\n",
      "Expected=0.667, Predicted=0.541\n",
      "Expected=0.500, Predicted=0.332\n",
      "Expected=0.833, Predicted=0.482\n",
      "Expected=0.500, Predicted=0.470\n",
      "Expected=0.333, Predicted=0.402\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression on wine quality dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'winequality-white.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "n_epoch = 50\n",
    "l_rate = 0.001\n",
    "\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "for row in dataset[:20]:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A k value of 5 was used for cross-validation, giving each fold 4,898/5 = 979.6 or just under 1000 records to be evaluated upon each iteration. A learning rate of 0.01 and 50 training epochs were chosen with a little experimentation.</p><p>You can try your own configurations and see if you can beat my score.</p><p>Running this example prints the scores for each of the 5 cross-validation folds&nbsp;then prints the mean RMSE.</p><p>We can see that the RMSE (on the normalized dataset) is 0.126, lower than the baseline value of 0.148 if we just predicted the mean (using the Zero Rule Algorithm).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Zero Rule Algorithm</h2><p><strong>Zero Rule</strong> or <strong>ZeroR</strong> is the benchmark procedure for classification algorithms whose output is simply the most frequently occurring classification in a set of data. If 65% of data items have that classification, ZeroR would presume that all data items have it and would be right 65% of the time.</p><p>ZeroR is a simple and effective benchmark: if an algorithm correctly predicts classifications less frequently than ZeroR, it is obviously of no value for the domain in question!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Review</h2><p>In this tutorial, you discovered how to implement linear regression using stochastic gradient descent from scratch with Python.</p><p>You learned.</p><ul>\n",
    "<li>How to make predictions for a multivariate linear regression problem.</li>\n",
    "<li>How to optimize a set of coefficients using stochastic gradient descent.</li>\n",
    "<li>How to apply the technique to a real regression predictive modeling problem.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.12248058224111809, 0.1303401750916616, 0.12620370547528478, 0.1289768795281032, 0.12446990678685776]\n",
      "Mean RMSE: 0.126\n"
     ]
    }
   ],
   "source": [
    "from random import seed, randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = []\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = []\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = len(dataset) // n_folds\n",
    "    for _ in range(n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = np.sum((np.array(predicted) - np.array(actual)) ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = []\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = [row[:-1] for row in fold]  # Exclude the target variable\n",
    "        actual = [row[-1] for row in fold]  # Get the actual target values\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        rmse = rmse_metric(actual, predicted)\n",
    "        scores.append(rmse)\n",
    "    return scores  \n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return yhat\n",
    " \n",
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for _ in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row[:-1], coef)  # Exclude the target variable\n",
    "            error = yhat - row[-1]  # Calculate the error\n",
    "            coef[0] = coef[0] - l_rate * error\n",
    "            for i in range(len(row) - 1):\n",
    "                coef[i + 1] = coef[i + 1] - l_rate * error * row[i]\n",
    "    return coef\n",
    " \n",
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def linear_regression_sgd(train, test, l_rate, n_epoch):\n",
    "    predictions = []\n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        predictions.append(yhat)\n",
    "    return predictions\n",
    "\n",
    "# Linear Regression on wine quality dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'winequality-white.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.01  # Adjust the learning rate\n",
    "n_epoch = 50 # Increase the number of epochs\n",
    "scores = evaluate_algorithm(dataset, linear_regression_sgd, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean RMSE: %.3f' % (sum(scores) / float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Split dataset into features and target\n",
    "def split_dataset(dataset):\n",
    "    X = [row[:-1] for row in dataset]\n",
    "    y = [row[-1] for row in dataset]\n",
    "    return X, y\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(X, y, n_folds, n_trees):\n",
    "    scores = []\n",
    "    for _ in range(n_folds):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Apply TomekLinks to balance the dataset\n",
    "        tl = TomekLinks()\n",
    "        X_train_resampled, y_train_resampled = tl.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Create and train the Random Forest model\n",
    "        model = RandomForestRegressor(n_estimators=n_trees, random_state=42)\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        scores.append(rmse)\n",
    "    return scores  \n",
    "\n",
    "# Linear Regression on wine quality dataset\n",
    "np.random.seed(1)\n",
    "\n",
    "# Load and prepare data\n",
    "filename = 'winequality-white.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# Split dataset into features and target\n",
    "X, y = split_dataset(dataset)\n",
    "\n",
    "# Evaluate algorithm\n",
    "n_folds = 5\n",
    "n_trees = 100  # Adjust the number of trees in the Random Forest\n",
    "scores = evaluate_algorithm(X, y, n_folds, n_trees)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean RMSE: %.3f' % (sum(scores) / float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
